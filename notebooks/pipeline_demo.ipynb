{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Customer Intelligence Pipeline — Interactive Workbench\n",
    "\n",
    "This notebook provides three capabilities:\n",
    "\n",
    "1. **End-to-end demo** — Run the full extraction pipeline on any transcript and inspect all 4 signal layers\n",
    "2. **Prompt tuning** — Edit extraction prompts, re-run individual layers, and compare results side-by-side\n",
    "3. **Evaluation** — Measure extraction quality against ground truth using the same metrics as the test suite\n",
    "\n",
    "### Prerequisites\n",
    "\n",
    "- `ANTHROPIC_API_KEY` set in environment or `.env` file\n",
    "- Synthetic corpus generated (`python -m customer_intelligence.synthetic.generator`)\n",
    "- Package dependencies installed (`pip install -e .`)\n",
    "\n",
    "### Cost awareness\n",
    "\n",
    "Each extraction layer makes one LLM API call to `claude-opus-4-6`. A full extraction is 3–4 calls.\n",
    "The corpus-level evaluation (Section 7) runs extraction on all 7 transcripts (~25 API calls total)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "import sys\n",
    "from pathlib import Path\n",
    "from datetime import datetime, timezone\n",
    "from IPython.display import display, HTML, Markdown\n",
    "\n",
    "import anthropic\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Paths relative to notebook location\n",
    "PROJECT_ROOT = Path(\"..\").resolve()\n",
    "DATA_DIR = PROJECT_ROOT / \"data\"\n",
    "TRANSCRIPTS_DIR = DATA_DIR / \"transcripts\"\n",
    "GROUND_TRUTH_DIR = DATA_DIR / \"ground_truth\"\n",
    "EXTRACTIONS_DIR = DATA_DIR / \"extractions\"\n",
    "PROMPTS_DIR = PROJECT_ROOT / \"src\" / \"customer_intelligence\" / \"extraction\" / \"prompts\"\n",
    "\n",
    "# Ensure the package is importable\n",
    "sys.path.insert(0, str(PROJECT_ROOT / \"src\"))\n",
    "\n",
    "from customer_intelligence.schemas.transcript import Transcript\n",
    "from customer_intelligence.schemas.extraction import ExtractionResult\n",
    "from customer_intelligence.schemas.surface import SurfaceSignals\n",
    "from customer_intelligence.schemas.behavioral import BehavioralSignals\n",
    "from customer_intelligence.schemas.psychographic import PsychographicSignals\n",
    "from customer_intelligence.schemas.multimodal import MultimodalSignals\n",
    "from customer_intelligence.schemas.summary import TranscriptSummary\n",
    "from customer_intelligence.extraction.extractor import (\n",
    "    extract,\n",
    "    extract_summary,\n",
    "    _extract_layer,\n",
    "    _format_transcript,\n",
    "    _coerce_to_schema,\n",
    "    _has_paralinguistic,\n",
    "    _parse_json_response,\n",
    "    MODEL,\n",
    ")\n",
    "from customer_intelligence.extraction.prompts import (\n",
    "    load_prompt,\n",
    "    SURFACE_EXTRACTION_PROMPT,\n",
    "    BEHAVIORAL_EXTRACTION_PROMPT,\n",
    "    PSYCHOGRAPHIC_EXTRACTION_PROMPT,\n",
    "    MULTIMODAL_DIVERGENCE_PROMPT,\n",
    "    SUMMARY_PROMPT,\n",
    ")\n",
    "\n",
    "load_dotenv(PROJECT_ROOT / \".env\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Anthropic client initialized\n",
      "Model: claude-opus-4-6\n",
      "Transcripts available: 10\n",
      "Ground truth available: 7\n",
      "Existing extractions: 1\n"
     ]
    }
   ],
   "source": [
    "client = anthropic.Anthropic()\n",
    "\n",
    "print(f\"Anthropic client initialized\")\n",
    "print(f\"Model: {MODEL}\")\n",
    "print(f\"Transcripts available: {len(list(TRANSCRIPTS_DIR.glob('*.json')))}\")\n",
    "print(f\"Ground truth available: {len(list(GROUND_TRUTH_DIR.glob('*.json')))}\")\n",
    "print(f\"Existing extractions: {len(list(EXTRACTIONS_DIR.glob('*.json')))}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Display Helpers\n",
    "\n",
    "Utility functions for rendering extraction results as formatted tables and structured output.\n",
    "These are used throughout the notebook — run this cell before any display calls."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Display helpers loaded.\n"
     ]
    }
   ],
   "source": [
    "def pretty_json(obj):\n",
    "    \"\"\"Pretty-print a Pydantic model or dict as formatted JSON.\"\"\"\n",
    "    if hasattr(obj, \"model_dump\"):\n",
    "        data = obj.model_dump()\n",
    "    else:\n",
    "        data = obj\n",
    "    print(json.dumps(data, indent=2, default=str))\n",
    "\n",
    "\n",
    "def format_table(headers: list[str], rows: list[list]) -> str:\n",
    "    \"\"\"Build a Markdown table string.\"\"\"\n",
    "    lines = [\"| \" + \" | \".join(headers) + \" |\"]\n",
    "    lines.append(\"| \" + \" | \".join([\"---\"] * len(headers)) + \" |\")\n",
    "    for row in rows:\n",
    "        lines.append(\"| \" + \" | \".join(str(c) for c in row) + \" |\")\n",
    "    return \"\\n\".join(lines)\n",
    "\n",
    "\n",
    "def display_table(headers, rows):\n",
    "    \"\"\"Render a Markdown table in the notebook.\"\"\"\n",
    "    display(Markdown(format_table(headers, rows)))\n",
    "\n",
    "\n",
    "def display_surface(surface: SurfaceSignals):\n",
    "    \"\"\"Display Layer 1 surface signals in formatted tables.\"\"\"\n",
    "    display(Markdown(\"### Aspect Sentiments\"))\n",
    "    rows = [[a.aspect, a.sentiment, f\"{a.intensity:.2f}\", (a.context or \"\")[:80]]\n",
    "            for a in surface.aspects]\n",
    "    display_table([\"Aspect\", \"Sentiment\", \"Intensity\", \"Context\"], rows)\n",
    "\n",
    "    display(Markdown(\"### Topics\"))\n",
    "    rows = [[t.name, t.timeline_position, f\"{t.relevance:.2f}\"]\n",
    "            for t in surface.topics]\n",
    "    display_table([\"Topic\", \"Position\", \"Relevance\"], rows)\n",
    "\n",
    "    display(Markdown(\"### Named Entities\"))\n",
    "    rows = [[e.name, e.entity_type, e.role or \"\", e.mention_count]\n",
    "            for e in surface.entities]\n",
    "    display_table([\"Name\", \"Type\", \"Role\", \"Mentions\"], rows)\n",
    "\n",
    "    display(Markdown(\"### Key Phrases\"))\n",
    "    rows = [[kp.phrase, f\"{kp.relevance:.2f}\", (kp.context or \"\")[:60]]\n",
    "            for kp in surface.key_phrases]\n",
    "    display_table([\"Phrase\", \"Relevance\", \"Context\"], rows)\n",
    "\n",
    "\n",
    "def display_behavioral(behavioral: BehavioralSignals):\n",
    "    \"\"\"Display Layer 2 behavioral signals.\"\"\"\n",
    "    display(Markdown(\"### Objection Triples\"))\n",
    "    for i, triple in enumerate(behavioral.objection_triples):\n",
    "        obj = triple.objection\n",
    "        res = triple.resolution\n",
    "        out = triple.outcome\n",
    "        display(Markdown(f\"**Objection {i+1}** ({obj.type}, {obj.conversation_stage})\"))\n",
    "        print(f\"  Speaker: {obj.speaker_role}\")\n",
    "        print(f\"  Language: \\\"{obj.specific_language}\\\"\")\n",
    "        if res:\n",
    "            print(f\"  Resolution: {res.type} — \\\"{res.specific_language}\\\"\")\n",
    "        print(f\"  Outcome: resolved={out.resolved}, progressed={out.deal_progressed}\")\n",
    "        print(f\"  Confidence: {triple.confidence:.2f}\")\n",
    "        print()\n",
    "\n",
    "    display(Markdown(\"### Buying Intent Markers\"))\n",
    "    rows = [[m.type, m.evidence[:80], f\"{m.confidence:.2f}\"]\n",
    "            for m in behavioral.buying_intent_markers]\n",
    "    display_table([\"Type\", \"Evidence\", \"Confidence\"], rows)\n",
    "\n",
    "    display(Markdown(\"### Competitive Mentions\"))\n",
    "    if behavioral.competitive_mentions:\n",
    "        rows = [[c.competitor, (c.context or \"\")[:60], c.sentiment]\n",
    "                for c in behavioral.competitive_mentions]\n",
    "        display_table([\"Competitor\", \"Context\", \"Sentiment\"], rows)\n",
    "    else:\n",
    "        print(\"  (none detected)\")\n",
    "\n",
    "    display(Markdown(\"### Engagement Trajectory\"))\n",
    "    rows = [[e.phase, e.participation_level, e.question_depth, e.energy, e.notes or \"\"]\n",
    "            for e in behavioral.engagement_trajectory]\n",
    "    display_table([\"Phase\", \"Participation\", \"Question Depth\", \"Energy\", \"Notes\"], rows)\n",
    "\n",
    "\n",
    "def display_psychographic(psychographic: PsychographicSignals):\n",
    "    \"\"\"Display Layer 3 psychographic signals.\"\"\"\n",
    "    mm = psychographic.mental_model\n",
    "    display(Markdown(\"### Mental Model\"))\n",
    "    print(f\"  Primary: {mm.primary}\")\n",
    "    print(f\"  Secondary: {mm.secondary}\")\n",
    "    print(f\"  Confidence: {mm.confidence:.2f}\")\n",
    "    print(f\"  Reasoning: {mm.reasoning}\")\n",
    "    print(f\"  Evidence: {mm.evidence}\")\n",
    "\n",
    "    display(Markdown(\"### Persona Indicators\"))\n",
    "    for p in psychographic.persona_indicators:\n",
    "        print(f\"  {p.archetype} (confidence={p.confidence:.2f})\")\n",
    "        print(f\"    Reasoning: {p.reasoning}\")\n",
    "        print(f\"    Evidence: {p.evidence}\")\n",
    "        print()\n",
    "\n",
    "    display(Markdown(\"### Language Fingerprint\"))\n",
    "    lf = psychographic.language_fingerprint\n",
    "    print(f\"  Vocabulary: {lf.distinctive_vocabulary}\")\n",
    "    print(f\"  Metaphors: {lf.metaphors}\")\n",
    "    print(f\"  Framing: {lf.framing_patterns}\")\n",
    "\n",
    "\n",
    "def display_multimodal(multimodal: MultimodalSignals | None):\n",
    "    \"\"\"Display Layer 4 multimodal signals.\"\"\"\n",
    "    if multimodal is None:\n",
    "        print(\"  (No multimodal signals — transcript lacks paralinguistic annotations)\")\n",
    "        return\n",
    "\n",
    "    display(Markdown(\"### Divergence Signals\"))\n",
    "    rows = [[d.utterance_index, d.type, d.text_sentiment,\n",
    "             \", \".join(d.nonverbal_cues), d.interpretation[:60], f\"{d.confidence:.2f}\"]\n",
    "            for d in multimodal.divergences]\n",
    "    display_table([\"Utterance\", \"Type\", \"Text Sentiment\", \"Cues\", \"Interpretation\", \"Confidence\"], rows)\n",
    "\n",
    "    display(Markdown(\"### Composite Sentiments\"))\n",
    "    rows = [[cs.utterance_index, cs.original_text_polarity,\n",
    "             cs.adjusted_polarity, f\"{cs.confidence:.2f}\", cs.note or \"\"]\n",
    "            for cs in multimodal.composite_sentiments]\n",
    "    display_table([\"Utterance\", \"Original\", \"Adjusted\", \"Confidence\", \"Note\"], rows)\n",
    "\n",
    "\n",
    "def display_result_summary(result: ExtractionResult):\n",
    "    \"\"\"One-line summary of an ExtractionResult.\"\"\"\n",
    "    display(Markdown(\n",
    "        f\"**{result.transcript_id}** | \"\n",
    "        f\"confidence={result.overall_confidence:.2f} | \"\n",
    "        f\"topics={len(result.surface.topics)} | \"\n",
    "        f\"entities={len(result.surface.entities)} | \"\n",
    "        f\"objections={len(result.behavioral.objection_triples)} | \"\n",
    "        f\"intent_markers={len(result.behavioral.buying_intent_markers)} | \"\n",
    "        f\"multimodal={'yes' if result.multimodal else 'no'}\"\n",
    "    ))\n",
    "    if result.notes:\n",
    "        for note in result.notes:\n",
    "            print(f\"  Note: {note}\")\n",
    "\n",
    "\n",
    "def display_summary(summary: TranscriptSummary):\n",
    "    \"\"\"Display a transcript summary with all sections.\"\"\"\n",
    "    display(Markdown(\"### Executive Summary\"))\n",
    "    display(Markdown(summary.executive_summary))\n",
    "\n",
    "    display(Markdown(\"### Key Moments\"))\n",
    "    for i, m in enumerate(summary.key_moments, 1):\n",
    "        display(Markdown(f\"**{i}. {m.moment_type.upper()}** (turns {m.turn_indices})\"))\n",
    "        print(f\"  {m.description}\")\n",
    "        print(f\"  Significance: {m.significance}\\n\")\n",
    "\n",
    "    display(Markdown(\"### Action Items\"))\n",
    "    if summary.action_items:\n",
    "        rows = [[a.action, a.owner, a.criticality] for a in summary.action_items]\n",
    "        display_table([\"Action\", \"Owner\", \"Criticality\"], rows)\n",
    "    else:\n",
    "        print(\"  (none identified)\")\n",
    "\n",
    "    display(Markdown(\"### Prospect Priorities\"))\n",
    "    for i, p in enumerate(summary.prospect_priorities, 1):\n",
    "        print(f\"  {i}. {p}\")\n",
    "\n",
    "    display(Markdown(\"### Concerns to Address\"))\n",
    "    if summary.concerns_to_address:\n",
    "        for i, c in enumerate(summary.concerns_to_address, 1):\n",
    "            print(f\"  {i}. {c}\")\n",
    "    else:\n",
    "        print(\"  (none identified)\")\n",
    "\n",
    "\n",
    "print(\"Display helpers loaded.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Data Loading\n",
    "\n",
    "Load transcripts and ground truth from the synthetic corpus. The corpus contains\n",
    "7 transcripts across 5 accounts, covering won/lost/stalled outcomes, with and\n",
    "without paralinguistic annotations.\n",
    "\n",
    "If you haven't generated the corpus yet, run:\n",
    "```bash\n",
    "python -m customer_intelligence.synthetic.generator\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 10 transcripts, 7 ground truths, 1 existing extractions\n"
     ]
    }
   ],
   "source": [
    "# Load all transcripts\n",
    "transcripts = {}\n",
    "for path in sorted(TRANSCRIPTS_DIR.glob(\"*.json\")):\n",
    "    t = Transcript.model_validate_json(path.read_text())\n",
    "    transcripts[t.call_metadata.call_id] = t\n",
    "\n",
    "# Load all ground truth\n",
    "ground_truths = {}\n",
    "for path in sorted(GROUND_TRUTH_DIR.glob(\"*.json\")):\n",
    "    gt = ExtractionResult.model_validate_json(path.read_text())\n",
    "    ground_truths[gt.transcript_id] = gt\n",
    "\n",
    "# Load any existing extractions\n",
    "extractions = {}\n",
    "EXTRACTIONS_DIR.mkdir(parents=True, exist_ok=True)\n",
    "for path in sorted(EXTRACTIONS_DIR.glob(\"*.json\")):\n",
    "    ex = ExtractionResult.model_validate_json(path.read_text())\n",
    "    extractions[ex.transcript_id] = ex\n",
    "\n",
    "print(f\"Loaded {len(transcripts)} transcripts, {len(ground_truths)} ground truths, {len(extractions)} existing extractions\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "### Corpus Overview"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "| Call ID | Company | Size | Outcome | Turns | Paralinguistic | Ground Truth |\n",
       "| --- | --- | --- | --- | --- | --- | --- |\n",
       "| cloudfirst_analytics_call1 | CloudFirst Analytics | smb | won | 28 | No | No |\n",
       "| growthco_call1 | GrowthCo | startup | won | 22 | No | Yes |\n",
       "| legacy_systems_corp_call1 | Legacy Systems Corp | enterprise | lost | 83 | No | Yes |\n",
       "| legacy_systems_corp_call2 | Legacy Systems Corp | enterprise | lost | 55 | No | Yes |\n",
       "| meridian_healthcare_call1 | Meridian Healthcare | enterprise | stalled | 48 | Yes | No |\n",
       "| safeguard_inc_call1 | SafeGuard Inc | enterprise | stalled | 49 | Yes | Yes |\n",
       "| scaleup_ltd_call1 | ScaleUp Ltd | smb | won | 36 | Yes | Yes |\n",
       "| scaleup_ltd_call2 | ScaleUp Ltd | smb | won | 27 | Yes | Yes |\n",
       "| techcorp_call1 | TechCorp | mid_market | won | 37 | Yes | Yes |\n",
       "| velocity_logistics_call1 | Velocity Logistics | mid_market | won | 37 | Yes | No |"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(Markdown(\"### Corpus Overview\"))\n",
    "\n",
    "rows = []\n",
    "for call_id, t in transcripts.items():\n",
    "    a = t.account\n",
    "    has_para = _has_paralinguistic(t)\n",
    "    has_gt = call_id in ground_truths\n",
    "    rows.append([\n",
    "        call_id,\n",
    "        a.company_name,\n",
    "        a.company_size,\n",
    "        a.deal_outcome,\n",
    "        len(t.utterances),\n",
    "        \"Yes\" if has_para else \"No\",\n",
    "        \"Yes\" if has_gt else \"No\",\n",
    "    ])\n",
    "\n",
    "display_table(\n",
    "    [\"Call ID\", \"Company\", \"Size\", \"Outcome\", \"Turns\", \"Paralinguistic\", \"Ground Truth\"],\n",
    "    rows,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "SELECTED_CALL_ID = list(transcripts.keys())[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected: cloudfirst_analytics_call1\n",
      "Company: CloudFirst Analytics (smb)\n",
      "Deal: discovery -> won\n",
      "Turns: 28\n",
      "Paralinguistic: False\n",
      "\n",
      "--- First 5 utterances ---\n",
      "[0] rep: Hi Nina, thanks for hopping on. I saw you downloaded our attribution benchmarking report last week. What caught your eye\n",
      "[1] prospect_head_of_data: Yeah, the section on multi-touch attribution gaps was exactly what we're wrestling with. We're a 60-person shop doing an\n",
      "[2] rep: Ha, the cobbler's children have no shoes. That's actually more common than you'd think with analytics firms. Tell me mor\n",
      "[3] prospect_head_of_data: We're running campaigns across LinkedIn, Google, our podcast sponsorships, and a bunch of partner co-marketing stuff. Ri\n",
      "[4] rep: Two days a month on manual attribution. And you said you don't trust the output. What breaks down specifically?\n"
     ]
    }
   ],
   "source": [
    "# === CHANGE THIS to select a different transcript ===\n",
    "SELECTED_CALL_ID = list(transcripts.keys())[0]\n",
    "\n",
    "transcript = transcripts[SELECTED_CALL_ID]\n",
    "print(f\"Selected: {SELECTED_CALL_ID}\")\n",
    "print(f\"Company: {transcript.account.company_name} ({transcript.account.company_size})\")\n",
    "print(f\"Deal: {transcript.account.deal_stage} -> {transcript.account.deal_outcome}\")\n",
    "print(f\"Turns: {len(transcript.utterances)}\")\n",
    "print(f\"Paralinguistic: {_has_paralinguistic(transcript)}\")\n",
    "print(f\"\\n--- First 5 utterances ---\")\n",
    "for u in transcript.utterances[:5]:\n",
    "    para_tag = \" [paralinguistic]\" if u.paralinguistic else \"\"\n",
    "    print(f\"[{u.turn_index}] {u.speaker}{para_tag}: {u.text[:120]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cloudfirst_analytics_call1\n"
     ]
    }
   ],
   "source": [
    "print(SELECTED_CALL_ID)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Understanding the Input: Transcript Structure\n",
    "\n",
    "Before running extraction, let's examine what a sales call transcript looks like.\n",
    "Each transcript contains three parts:\n",
    "- **Account context** — company profile, deal stage, stakeholders\n",
    "- **Call metadata** — date, duration, participants\n",
    "- **Utterances** — speaker-labeled conversation turns with optional paralinguistic annotations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "### Account Context"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "| Field | Value |\n",
       "| --- | --- |\n",
       "| Company | CloudFirst Analytics |\n",
       "| Size | smb |\n",
       "| Industry | Data Analytics |\n",
       "| Deal Stage | discovery |\n",
       "| Deal Outcome | won |\n",
       "| Stakeholders | Nina Patel (Head of Data, executive_champion) |"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "### Call Metadata"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "| Field | Value |\n",
       "| --- | --- |\n",
       "| Call ID | cloudfirst_analytics_call1 |\n",
       "| Date | 2026-02-07 |\n",
       "| Duration | 22 minutes |\n",
       "| Call # | 1 |\n",
       "| Participants | rep, prospect_head_of_data |\n",
       "| Total Turns | 28 |"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "<details>\n",
       "<summary><b>Raw Transcript JSON</b> (click to expand — 8964 chars)</summary>\n",
       "<pre>{\n",
       "  \"account\": {\n",
       "    \"company_name\": \"CloudFirst Analytics\",\n",
       "    \"company_size\": \"smb\",\n",
       "    \"industry\": \"Data Analytics\",\n",
       "    \"deal_stage\": \"discovery\",\n",
       "    \"deal_outcome\": \"won\",\n",
       "    \"stakeholders\": [\n",
       "      {\n",
       "        \"name\": \"Nina Patel\",\n",
       "        \"role\": \"Head of Data\",\n",
       "        \"persona_type\": \"executive_champion\"\n",
       "      }\n",
       "    ]\n",
       "  },\n",
       "  \"call_metadata\": {\n",
       "    \"call_id\": \"cloudfirst_analytics_call1\",\n",
       "    \"call_date\": \"2026-02-07\",\n",
       "    \"call_number\": 1,\n",
       "    \"duration_minutes\": 22,\n",
       "    \"participants\": [\n",
       "      \"rep\",\n",
       "      \"prospect_head_of_data\"\n",
       "    ]\n",
       "  },\n",
       "  \"utterances\": [\n",
       "    {\n",
       "      \"speaker\": \"rep\",\n",
       "      \"text\": \"Hi Nina, thanks for hopping on. I saw you downloaded our attribution benchmarking report last week. What caught your eye?\",\n",
       "      \"turn_index\": 0,\n",
       "      \"paralinguistic\": null\n",
       "    },\n",
       "    {\n",
       "      \"speaker\": \"prospect_head_of_data\",\n",
       "      \"text\": \"Yeah, the section on multi-touch attribution gaps was exactly what we're wrestling with. We're a 60-person shop doing analytics consulting, and our own marketing attribution is embarrassingly bad.\",\n",
       "      \"turn_index\": 1,\n",
       "      \"paralinguistic\": null\n",
       "    },\n",
       "    {\n",
       "      \"speaker\": \"rep\",\n",
       "      \"text\": \"Ha, the cobbler's children have no shoes. That's actually more common than you'd think with analytics firms. Tell me more about what you're seeing.\",\n",
       "      \"turn_index\": 2,\n",
       "      \"paralinguistic\": null\n",
       "    },\n",
       "    {\n",
       "      \"speaker\": \"prospect_head_of_data\",\n",
       "      \"text\": \"We're running campaigns across LinkedIn, Google, our podcast sponsorships, and a bunch of partner co-marketing stuff. Right now we're stitching everything together in spreadsheets. My analyst spends probably two days a month just building the attribution report, and honestly, I don't trust the numbers.\",\n",
       "      \"turn_index\": 3,\n",
       "      \"paralinguistic\": null\n",
       "    },\n",
       "    {\n",
       "      \"speaker\": \"rep\",\n",
       "      \"text\": \"Two days a month on manual attribution. And you said you don't trust the output. What breaks down specifically?\",\n",
       "      \"turn_index\": 4,\n",
       "      \"paralinguistic\": null\n",
       "    },\n",
       "    {\n",
       "      \"speaker\": \"prospect_head_of_data\",\n",
       "      \"text\": \"The big one is we can't track the podcast-to-website-to-demo-request journey. People hear us on a podcast, Google us a week later, and we attribute it all to organic search. We know the podcasts are working because prospects mention them on calls, but the data doesn't show it.\",\n",
       "      \"turn_index\": 5,\n",
       "      \"paralinguistic\": null\n",
       "    },\n",
       "    {\n",
       "      \"speaker\": \"rep\",\n",
       "      \"text\": \"That's a classic dark funnel problem. The offline-to-online handoff is where most attribution models fall apart. How much are you spending annually on those podcast sponsorships?\",\n",
       "      \"turn_index\": 6,\n",
       "      \"paralinguistic\": null\n",
       "    },\n",
       "    {\n",
       "      \"speaker\": \"prospect_head_of_data\",\n",
       "      \"text\": \"About 180K a year. And our CEO keeps asking me whether we should double down or kill them. I can't give her a straight answer with the data I have.\",\n",
       "      \"turn_index\": 7,\n",
       "      \"paralinguistic\": null\n",
       "    },\n",
       "    {\n",
       "      \"sp...</pre>\n",
       "</details>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(Markdown(\"### Account Context\"))\n",
    "a = transcript.account\n",
    "display_table(\n",
    "    [\"Field\", \"Value\"],\n",
    "    [\n",
    "        [\"Company\", a.company_name],\n",
    "        [\"Size\", a.company_size],\n",
    "        [\"Industry\", a.industry],\n",
    "        [\"Deal Stage\", a.deal_stage],\n",
    "        [\"Deal Outcome\", a.deal_outcome],\n",
    "        [\"Stakeholders\", \", \".join(f\"{s.name} ({s.role}, {s.persona_type})\" for s in a.stakeholders)],\n",
    "    ]\n",
    ")\n",
    "\n",
    "display(Markdown(\"### Call Metadata\"))\n",
    "cm = transcript.call_metadata\n",
    "display_table(\n",
    "    [\"Field\", \"Value\"],\n",
    "    [\n",
    "        [\"Call ID\", cm.call_id],\n",
    "        [\"Date\", cm.call_date],\n",
    "        [\"Duration\", f\"{cm.duration_minutes} minutes\"],\n",
    "        [\"Call #\", str(cm.call_number)],\n",
    "        [\"Participants\", \", \".join(cm.participants)],\n",
    "        [\"Total Turns\", str(len(transcript.utterances))],\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Collapsible raw JSON\n",
    "raw_transcript = json.dumps(transcript.model_dump(), indent=2, default=str)\n",
    "display(HTML(f\"\"\"\n",
    "<details>\n",
    "<summary><b>Raw Transcript JSON</b> (click to expand — {len(raw_transcript)} chars)</summary>\n",
    "<pre>{raw_transcript[:3000]}{\"...\" if len(raw_transcript) > 3000 else \"\"}</pre>\n",
    "</details>\n",
    "\"\"\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "### The Conversation: Readable View"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "The raw input to the extraction pipeline — speaker-labeled turns with optional paralinguistic annotations."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**rep** (turn 0) "
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Hi Nina, thanks for hopping on. I saw you downloaded our attribution benchmarking report last week. What caught your eye?\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "**prospect_head_of_data** (turn 1) "
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Yeah, the section on multi-touch attribution gaps was exactly what we're wrestling with. We're a 60-person shop doing analytics consulting, and our own marketing attribution is embarrassingly bad.\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "**rep** (turn 2) "
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Ha, the cobbler's children have no shoes. That's actually more common than you'd think with analytics firms. Tell me more about what you're seeing.\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "**prospect_head_of_data** (turn 3) "
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  We're running campaigns across LinkedIn, Google, our podcast sponsorships, and a bunch of partner co-marketing stuff. Right now we're stitching everything together in spreadsheets. My analyst spends probably two days a month just building the attribution report, and honestly, I don't trust the numbers.\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "**rep** (turn 4) "
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Two days a month on manual attribution. And you said you don't trust the output. What breaks down specifically?\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "**prospect_head_of_data** (turn 5) "
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  The big one is we can't track the podcast-to-website-to-demo-request journey. People hear us on a podcast, Google us a week later, and we attribute it all to organic search. We know the podcasts are working because prospects mention them on calls, but the data doesn't show it.\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "**rep** (turn 6) "
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  That's a classic dark funnel problem. The offline-to-online handoff is where most attribution models fall apart. How much are you spending annually on those podcast sponsorships?\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "**prospect_head_of_data** (turn 7) "
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  About 180K a year. And our CEO keeps asking me whether we should double down or kill them. I can't give her a straight answer with the data I have.\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "**rep** (turn 8) "
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  So you have a 180K spending decision that's essentially flying blind. That's actually a perfect use case for us. Our self-reported attribution module captures that exact journey. When someone books a demo, they get a 'how did you hear about us' field that ties back into the attribution model alongside the digital touchpoints.\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "**prospect_head_of_data** (turn 9) "
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  We've tried the how-did-you-hear-about-us thing manually. The data quality was terrible. People just pick the first option.\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "**rep** (turn 10) "
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Totally. The trick is in how you present it and how you weight it. We use a free-text field instead of a dropdown, then use NLP to categorize and cross-reference with digital touchpoint data. Clients typically see 70 to 80% match rates between self-reported and digital signals.\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "**prospect_head_of_data** (turn 11) "
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  That's clever. Okay, what does something like this cost for a company our size? We're not a big budget operation.\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "**rep** (turn 12) "
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  For your team size and channel mix, you'd be looking at our Starter tier. That's 42K annually, which includes the core attribution engine, the self-reported module, and integrations with your ad platforms and CRM.\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "**prospect_head_of_data** (turn 13) "
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  42K. That's not nothing, but if it saves my analyst two days a month and helps us make a smarter call on 180K in podcast spend, the math works pretty quickly.\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "**rep** (turn 14) "
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Exactly. And that analyst time is probably the smaller piece. The real value is having confidence in your channel allocation decisions. If podcasts are actually driving 30% of your pipeline but your current data says 5%, that changes your entire strategy.\n",
      "\n",
      "[Showing first 15 of 28 total turns]\n"
     ]
    }
   ],
   "source": [
    "display(Markdown(\"### The Conversation: Readable View\"))\n",
    "display(Markdown(\"The raw input to the extraction pipeline — speaker-labeled turns with optional paralinguistic annotations.\"))\n",
    "\n",
    "for u in transcript.utterances[:15]:\n",
    "    speaker_label = f\"**{u.speaker}** (turn {u.turn_index})\"\n",
    "\n",
    "    para_tags = []\n",
    "    if u.paralinguistic:\n",
    "        p = u.paralinguistic\n",
    "        if p.pause_before_sec:\n",
    "            para_tags.append(f\"*{p.pause_before_sec}s pause*\")\n",
    "        if p.energy:\n",
    "            para_tags.append(f\"*{p.energy} energy*\")\n",
    "        if p.tone:\n",
    "            para_tags.append(f\"*{p.tone}*\")\n",
    "        if p.behaviors:\n",
    "            para_tags.append(f\"*{', '.join(p.behaviors)}*\")\n",
    "\n",
    "    para_str = \" \".join(para_tags) if para_tags else \"\"\n",
    "    display(Markdown(f\"{speaker_label} {para_str}\"))\n",
    "    print(f\"  {u.text}\\n\")\n",
    "\n",
    "print(f\"[Showing first 15 of {len(transcript.utterances)} total turns]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What We Extract: The 4 Signal Layers\n",
    "\n",
    "From this raw conversation, the pipeline extracts structured intelligence at 4 levels:\n",
    "\n",
    "| Layer | What It Captures | Example Output |\n",
    "|-------|------------------|----------------|\n",
    "| **Surface** | Aspect sentiment, topics, entities, key phrases | `pricing: negative (0.7)`, `product: positive (0.8)` |\n",
    "| **Behavioral** | Objection triples, buying intent, competitive mentions, engagement | `Objection: pricing → Resolution: ROI argument → Outcome: resolved` |\n",
    "| **Psychographic** | Mental models, buyer personas, language fingerprint | `Mental model: cost_reduction`, `Persona: analytical_evaluator` |\n",
    "| **Multimodal** | Text-audio divergence, composite sentiment | `Said \"fine\" but [2.1s pause + falling pitch] = hidden concern` |\n",
    "\n",
    "Let's run the extraction and see these layers in action."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## End-to-End Pipeline Demo\n",
    "\n",
    "Run the full 4-layer extraction pipeline on the selected transcript.\n",
    "This makes 3–4 LLM API calls (surface, behavioral, psychographic, and\n",
    "optionally multimodal if paralinguistic annotations are present).\n",
    "\n",
    "| Layer | Signals | Always runs? |\n",
    "|-------|---------|-------------|\n",
    "| Surface | Aspect sentiment, topics, entities, key phrases | Yes |\n",
    "| Behavioral | Objection triples, buying intent, competitive mentions, engagement trajectory | Yes |\n",
    "| Psychographic | Mental model, persona indicators, language fingerprint | Yes |\n",
    "| Multimodal | Text-audio divergences, composite sentiments | Only if paralinguistic annotations present |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extraction complete for cloudfirst_analytics_call1\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "**cloudfirst_analytics_call1** | confidence=0.83 | topics=12 | entities=6 | objections=4 | intent_markers=9 | multimodal=no"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Note: No paralinguistic annotations — multimodal extraction skipped\n",
      "CPU times: user 61.7 ms, sys: 10.1 ms, total: 71.8 ms\n",
      "Wall time: 1min 32s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "result = extract(transcript, client=client)\n",
    "print(f\"Extraction complete for {result.transcript_id}\")\n",
    "display_result_summary(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "## Layer 1: Surface Signals"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "### Aspect Sentiments"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "| Aspect | Sentiment | Intensity | Context |\n",
       "| --- | --- | --- | --- |\n",
       "| current attribution process | negative | 0.85 | Manual spreadsheet-based attribution taking two days per month with untrusted ou |\n",
       "| podcast attribution / dark funnel tracking | negative | 0.90 | Cannot track podcast-to-website-to-demo journey; $180K spend decision flying bli |\n",
       "| self-reported attribution (manual/dropdown approach) | negative | 0.70 | Previously tried how-did-you-hear-about-us manually but data quality was terribl |\n",
       "| self-reported attribution (NLP-based product approach) | positive | 0.70 | Prospect found the free-text + NLP cross-referencing approach clever; 70-80% mat |\n",
       "| pricing | mixed | 0.60 | 42K annually acknowledged as 'not nothing' but prospect quickly rationalized ROI |\n",
       "| ROI / value proposition | positive | 0.85 | Prospect self-computed ROI: saving analyst time plus making smarter call on $180 |\n",
       "| integration (HubSpot / Google Ads / LinkedIn) | positive | 0.80 | Native HubSpot integration with one-click OAuth, plug-and-play Google Ads, UTM-b |\n",
       "| implementation timeline | positive | 0.75 | 3-4 weeks to go live perceived as fast; could kick off next Monday and be live b |\n",
       "| engineering resource requirements | positive | 0.85 | Marketing ops can handle entire setup without engineering cycles; described as ' |\n",
       "| historical data ingestion | positive | 0.80 | Can ingest up to 12 months of historical data from HubSpot and ad platforms; pro |\n",
       "| overall product impression | positive | 0.85 | Prospect said 'I'm pretty sold on this honestly' and plans to recommend moving f |\n",
       "| internal buying process | positive | 0.75 | CEO trusts prospect's judgment on data tooling; expects straightforward approval |"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "### Topics"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "| Topic | Position | Relevance |\n",
       "| --- | --- | --- |\n",
       "| Multi-touch attribution challenges | early | 1.00 |\n",
       "| Manual reporting inefficiency (spreadsheet-based process) | early | 0.85 |\n",
       "| Dark funnel / offline-to-online attribution | early | 0.95 |\n",
       "| Podcast sponsorship ROI and spend justification | early | 0.90 |\n",
       "| Self-reported attribution methodology (NLP-based) | mid | 0.80 |\n",
       "| Pricing and ROI calculation | mid | 0.85 |\n",
       "| Channel allocation strategy and confidence | mid | 0.80 |\n",
       "| Technical integration and implementation | mid | 0.80 |\n",
       "| Engineering resource constraints | mid | 0.70 |\n",
       "| Historical data ingestion and retroactive modeling | mid | 0.75 |\n",
       "| Next steps and procurement / internal approval | late | 0.85 |\n",
       "| Case studies and social proof | late | 0.60 |"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "### Named Entities"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "| Name | Type | Role | Mentions |\n",
       "| --- | --- | --- | --- |\n",
       "| Nina | person | Prospect, Head of Data at analytics consulting firm | 2 |\n",
       "| Alisha | person | CEO of prospect's company; final decision-maker | 2 |\n",
       "| HubSpot | product | Prospect's CRM platform; integration target | 4 |\n",
       "| Google Ads | product | Prospect's advertising platform; integration target | 2 |\n",
       "| LinkedIn | company | Prospect's advertising/campaign channel | 2 |\n",
       "| Google | company | Search engine referenced in attribution journey context | 1 |"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "### Key Phrases"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "| Phrase | Relevance | Context |\n",
       "| --- | --- | --- |\n",
       "| multi-touch attribution gaps | 1.00 | Core pain point that brought prospect to the conversation; d |\n",
       "| dark funnel | 0.95 | Rep's characterization of the offline-to-online tracking pro |\n",
       "| podcast-to-website-to-demo-request journey | 0.90 | Specific attribution gap prospect cannot track; listeners Go |\n",
       "| 180K annual podcast spend | 0.90 | Major budget decision at stake; CEO asking whether to double |\n",
       "| self-reported attribution module | 0.85 | Product feature using free-text NLP to capture how-did-you-h |\n",
       "| two days a month | 0.80 | Analyst time spent on manual attribution reporting; quantifi |\n",
       "| 42K annually | 0.85 | Starter tier pricing for prospect's company size and channel |\n",
       "| channel allocation decisions | 0.80 | Primary strategic value proposition; confidence in where to  |\n",
       "| one-click OAuth connection | 0.65 | HubSpot integration simplicity; no engineering needed |\n",
       "| 12 months of historical data | 0.75 | Retroactive data ingestion capability that excited prospect  |\n",
       "| three to four weeks implementation | 0.70 | Go-live timeline perceived favorably by prospect |\n",
       "| no spare engineering cycles | 0.70 | Key constraint for prospect; implementation must be handled  |\n",
       "| attribution benchmarking report | 0.60 | Content asset that initiated the sales conversation |\n",
       "| cobbler's children have no shoes | 0.40 | Rep's rapport-building acknowledgment that analytics firms o |\n",
       "| answer by Friday | 0.80 | Prospect's commitment to a decision timeline; strong buying  |"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "---\n",
       "## Layer 2: Behavioral Signals"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "### Objection Triples"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**Objection 1** (risk, mid)"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Speaker: prospect_head_of_data\n",
      "  Language: \"We've tried the how-did-you-hear-about-us thing manually. The data quality was terrible. People just pick the first option.\"\n",
      "  Resolution: technical_demo — \"The trick is in how you present it and how you weight it. We use a free-text field instead of a dropdown, then use NLP to categorize and cross-reference with digital touchpoint data. Clients typically see 70 to 80% match rates between self-reported and digital signals.\"\n",
      "  Outcome: resolved=True, progressed=True\n",
      "  Confidence: 0.95\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "**Objection 2** (pricing, mid)"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Speaker: prospect_head_of_data\n",
      "  Language: \"42K. That's not nothing, but if it saves my analyst two days a month and helps us make a smarter call on 180K in podcast spend, the math works pretty quickly.\"\n",
      "  Resolution: roi_argument — \"Exactly. And that analyst time is probably the smaller piece. The real value is having confidence in your channel allocation decisions. If podcasts are actually driving 30% of your pipeline but your current data says 5%, that changes your entire strategy.\"\n",
      "  Outcome: resolved=True, progressed=True\n",
      "  Confidence: 0.85\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "**Objection 3** (implementation, mid)"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Speaker: prospect_head_of_data\n",
      "  Language: \"That's really important to us. We don't have spare engineering cycles.\"\n",
      "  Resolution: risk_mitigation — \"Marketing ops can handle the whole thing. The HubSpot integration is a one-click OAuth connection. We handle the data pipeline setup on our end.\"\n",
      "  Outcome: resolved=True, progressed=True\n",
      "  Confidence: 0.92\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "**Objection 4** (authority, late)"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Speaker: prospect_head_of_data\n",
      "  Language: \"I need to loop in our CEO, Alisha, but I'm going to recommend we do this.\"\n",
      "  Resolution: social_proof — \"Absolutely. I have two great case studies from similar-sized consulting firms. I'll package those with the proposal. Would it be helpful to have a short call with Alisha as well, or do you think you can handle it internally?\"\n",
      "  Outcome: resolved=True, progressed=True\n",
      "  Confidence: 0.90\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "### Buying Intent Markers"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "| Type | Evidence | Confidence |\n",
       "| --- | --- | --- |\n",
       "| budget_confirmation | Okay, what does something like this cost for a company our size? We're not a big | 0.80 |\n",
       "| if_to_when_shift | 42K. That's not nothing, but if it saves my analyst two days a month and helps u | 0.85 |\n",
       "| timeline_question | Right. Okay, what does implementation look like? We run HubSpot and Google Ads p | 0.90 |\n",
       "| implementation_detail | Three to four weeks is fast. Do we need engineering resources or can my marketin | 0.95 |\n",
       "| implementation_detail | What about data we've already collected? Can we do any historical attribution or | 0.92 |\n",
       "| if_to_when_shift | Oh that's great. That would let us test the accuracy before we commit to changin | 0.95 |\n",
       "| next_steps_request | What would next steps look like if we wanted to move forward? | 0.99 |\n",
       "| stakeholder_introduction | I need to loop in our CEO, Alisha, but I'm going to recommend we do this. | 0.95 |\n",
       "| budget_confirmation | She trusts my judgment on data tooling. If she has questions I'll pull you in, b | 0.90 |"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "### Competitive Mentions"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (none detected)\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "### Engagement Trajectory"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "| Phase | Participation | Question Depth | Energy | Notes |\n",
       "| --- | --- | --- | --- | --- |\n",
       "| early | high | deep | high | Prospect immediately self-identified a specific pain point (multi-touch attribution gaps), volunteered detailed context about team size, channels, manual processes, and time cost. Showed vulnerability by calling their own attribution 'embarrassingly bad.' Very forthcoming with specifics like analyst time (two days/month) and distrust of numbers. |\n",
       "| mid | high | deep | high | Prospect pushed back constructively on self-reported attribution (showing sophistication), proactively asked about pricing, and self-justified the ROI ('the math works pretty quickly'). Transitioned rapidly from pricing to implementation questions, showing accelerating momentum. Asked about engineering resource requirements and historical data ingestion — both deep, practical questions indicating serious evaluation. |\n",
       "| late | high | moderate | high | Prospect explicitly stated 'I'm pretty sold on this honestly,' asked for next steps unprompted, volunteered a timeline commitment ('aim to have an answer by Friday'), and expressed confidence in internal approval ('She trusts my judgment on data tooling'). Energy remained high through close with humor ('you and me both'). Champion behavior is clear — prospect is self-qualifying and preparing to sell internally. |"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "---\n",
       "## Layer 3: Psychographic Signals"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "### Mental Model"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Primary: efficiency\n",
      "  Secondary: risk_mitigation\n",
      "  Confidence: 0.90\n",
      "  Reasoning: Nina's primary concern is efficiency: she repeatedly highlights the time her analyst wastes on manual spreadsheet work (two days a month), the lack of engineering cycles, and the need for a tool that marketing ops can handle without engineering involvement. She frames the ROI calculation around time savings and better decision-making speed. Her secondary model is risk_mitigation — she is deeply uncomfortable making a $180K budget decision without trustworthy data. She doesn't frame the problem as 'grow revenue' but rather as 'avoid making the wrong call.' She wants to validate attribution accuracy with historical data before committing to budget changes ('test the accuracy before we commit to changing our budget allocation'). She's not trying to expand capabilities or grow into new markets; she's trying to stop flying blind and reduce the risk of misallocating spend. While there's an element of cost_reduction (the $180K podcast decision), she frames it as needing confidence and accuracy rather than purely cutting costs — it's about making the right decision, not necessarily spending less.\n",
      "  Evidence: [\"My analyst spends probably two days a month just building the attribution report, and honestly, I don't trust the numbers.\", \"42K. That's not nothing, but if it saves my analyst two days a month and helps us make a smarter call on 180K in podcast spend, the math works pretty quickly.\", \"That's really important to us. We don't have spare engineering cycles.\", 'That would let us test the accuracy before we commit to changing our budget allocation.', \"And our CEO keeps asking me whether we should double down or kill them. I can't give her a straight answer with the data I have.\"]\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "### Persona Indicators"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  analytical_evaluator (confidence=0.85)\n",
      "    Reasoning: Nina asks detailed, specific technical questions in a methodical sequence: cost, implementation timeline, engineering requirements, historical data capabilities, and integration specifics. She pushes back on the self-reported attribution approach based on prior experience with data quality, showing she evaluates claims critically rather than accepting them at face value. She does mental ROI math on the spot (42K vs. 2 days/month + 180K decision). She wants to validate the tool's accuracy against historical data before making strategic changes — a classic analytical evaluator behavior.\n",
      "    Evidence: [\"We've tried the how-did-you-hear-about-us thing manually. The data quality was terrible. People just pick the first option.\", 'What does implementation look like? We run HubSpot and Google Ads primarily.', 'Do we need engineering resources or can my marketing ops person handle it?', 'Can we do any historical attribution or is it forward-looking only?', 'That would let us test the accuracy before we commit to changing our budget allocation.', \"42K. That's not nothing, but if it saves my analyst two days a month and helps us make a smarter call on 180K in podcast spend, the math works pretty quickly.\"]\n",
      "\n",
      "  executive_champion (confidence=0.45)\n",
      "    Reasoning: Nina shows some executive champion traits: she's clearly the internal decision driver for data tooling and expresses confidence in her ability to champion this purchase to the CEO without needing the rep's direct involvement. She moves quickly toward a decision and sets a Friday deadline. However, her confidence score is moderate because she's more analytical than visionary — she's not focused on strategic narrative or peer social proof in an executive sense, but she does request case studies as ammunition, which is a champion behavior.\n",
      "    Evidence: [\"I'm going to recommend we do this.\", \"She trusts my judgment on data tooling. If she has questions I'll pull you in, but honestly this should be straightforward.\", 'I think I can handle it.']\n",
      "\n",
      "  reluctant_adopter (confidence=0.15)\n",
      "    Reasoning: There are faint signals of budget sensitivity, but Nina is far from reluctant. She proactively moves toward next steps, expresses enthusiasm ('I'm pretty sold on this honestly'), and her caution around budget is rational rather than anxiety-driven. She was not brought in by someone else — she initiated engagement by downloading the report. Very low confidence for this archetype.\n",
      "    Evidence: [\"We're not a big budget operation.\", \"42K. That's not nothing.\"]\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "### Language Fingerprint"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Vocabulary: ['dark funnel', 'stitching together', 'flying blind', 'spare engineering cycles', 'budget allocation', 'channel mix', 'the math works', 'data quality']\n",
      "  Metaphors: [\"the cobbler's children have no shoes (acknowledged/adopted from rep)\", 'flying blind (used implicitly via rep, accepted framing)', 'stitching everything together in spreadsheets', 'get you off those spreadsheets (echoed by rep, endorsed)']\n",
      "  Framing: ['If X saves Y and helps Z, the math works — ROI justification through concrete arithmetic', \"We've tried [approach] manually. [Negative outcome]. — Prior experience as credibility filter\", \"That's really important to us. We don't have [resource]. — Constraint-first evaluation of feasibility\", \"I can't give her a straight answer with the data I have. — Framing problems as inability to act due to data gaps\", 'That would let us test [claim] before we commit to [action]. — Validation-before-commitment pattern', \"I'm pretty sold on this honestly. — Direct emotional signaling after analytical validation\"]\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "---\n",
       "## Layer 4: Multimodal Signals"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (No multimodal signals — transcript lacks paralinguistic annotations)\n"
     ]
    }
   ],
   "source": [
    "display(Markdown(\"## Layer 1: Surface Signals\"))\n",
    "display_surface(result.surface)\n",
    "\n",
    "display(Markdown(\"---\\n## Layer 2: Behavioral Signals\"))\n",
    "display_behavioral(result.behavioral)\n",
    "\n",
    "display(Markdown(\"---\\n## Layer 3: Psychographic Signals\"))\n",
    "display_psychographic(result.psychographic)\n",
    "\n",
    "display(Markdown(\"---\\n## Layer 4: Multimodal Signals\"))\n",
    "display_multimodal(result.multimodal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<details>\n",
       "<summary><b>Raw JSON output</b> (click to expand)</summary>\n",
       "<pre>{\n",
       "  \"transcript_id\": \"cloudfirst_analytics_call1\",\n",
       "  \"extraction_timestamp\": \"2026-02-08T08:30:34.899958+00:00\",\n",
       "  \"surface\": {\n",
       "    \"aspects\": [\n",
       "      {\n",
       "        \"aspect\": \"current attribution process\",\n",
       "        \"sentiment\": \"negative\",\n",
       "        \"intensity\": 0.85,\n",
       "        \"context\": \"Manual spreadsheet-based attribution taking two days per month with untrusted output; described as 'embarrassingly bad'\",\n",
       "        \"source_utterance_indices\": [\n",
       "          1,\n",
       "          3,\n",
       "          4\n",
       "        ]\n",
       "      },\n",
       "      {\n",
       "        \"aspect\": \"podcast attribution / dark funnel tracking\",\n",
       "        \"sentiment\": \"negative\",\n",
       "        \"intensity\": 0.9,\n",
       "        \"context\": \"Cannot track podcast-to-website-to-demo journey; $180K spend decision flying blind; all podcast-driven traffic misattributed to organic search\",\n",
       "        \"source_utterance_indices\": [\n",
       "          5,\n",
       "          6,\n",
       "          7\n",
       "        ]\n",
       "      },\n",
       "      {\n",
       "        \"aspect\": \"self-reported attribution (manual/dropdown approach)\",\n",
       "        \"sentiment\": \"negative\",\n",
       "        \"intensity\": 0.7,\n",
       "        \"context\": \"Previously tried how-did-you-hear-about-us manually but data quality was terrible; people pick first option\",\n",
       "        \"source_utterance_indices\": [\n",
       "          9\n",
       "        ]\n",
       "      },\n",
       "      {\n",
       "        \"aspect\": \"self-reported attribution (NLP-based product approach)\",\n",
       "        \"sentiment\": \"positive\",\n",
       "        \"intensity\": 0.7,\n",
       "        \"context\": \"Prospect found the free-text + NLP cross-referencing approach clever; 70-80% match rates cited\",\n",
       "        \"source_utterance_indices\": [\n",
       "          10,\n",
       "          11\n",
       "        ]\n",
       "      },\n",
       "      {\n",
       "        \"aspect\": \"pricing\",\n",
       "        \"sentiment\": \"mixed\",\n",
       "        \"intensity\": 0.6,\n",
       "        \"context\": \"42K annually acknowledged as 'not nothing' but prospect quickly rationalized ROI against analyst time savings and $180K podcast spend decision\",\n",
       "        \"source_utterance_indices\": [\n",
       "          11,\n",
       "          12,\n",
       "          13\n",
       "        ]\n",
       "      },\n",
       "      {\n",
       "        \"aspect\": \"ROI / value proposition\",\n",
       "        \"sentiment\": \"positive\",\n",
       "        \"intensity\": 0.85,\n",
       "        \"context\": \"Prospect self-computed ROI: saving analyst time plus making smarter call on $180K spend; rep reinforced confidence in channel allocation as bigger value\",\n",
       "        \"source_utterance_indices\": [\n",
       "          13,\n",
       "          14\n",
       "        ]\n",
       "      },\n",
       "      {\n",
       "        \"aspect\": \"integration (HubSpot / Google Ads / LinkedIn)\",\n",
       "        \"sentiment\": \"positive\",\n",
       "        \"intensity\": 0.8,\n",
       "        \"context\": \"Native HubSpot integration with one-click OAuth, plug-and-play Google Ads, UTM-based LinkedIn connector; no engineering resources needed\",\n",
       "        \"source_utterance_indices\": [\n",
       "          15,\n",
       "          16,\n",
       "          17,\n",
       "          18\n",
       "        ]\n",
       "      },\n",
       "      {\n",
       "        \"aspect\": \"implementation timeline\",\n",
       "        \"sentiment\": \"positive\",\n",
       "        \"intensity\": 0.75,\n",
       "        \"context\": \"3-4 weeks to go live perceived as fast; could kick off next Monday and be live by end of March\",\n",
       "        \"source_utterance_indices\": [\n",
       "          16,\n",
       "          17,\n",
       "          22\n",
       "        ]\n",
       "      },\n",
       "      {\n",
       "        \"aspect\": \"engineering resource requirements\",\n",
       "        \"sentiment\": \"positive\",\n",
       "        \"intensity\": 0.85,\n",
       "        \"context\": \"Marketing ops can handle entire setup without engineering cycles; described as 'really important' since no spare engineering capacity\",\n",
       "        \"source_utterance_indices\": [\n",
       "          17,\n",
       "          18,\n",
       "          19\n",
       "        ]\n",
       "      },\n",
       "      {\n",
       "        \"aspect\": \"historical data ingestion\",\n",
       "        \"sentiment\": \"positive\",\n",
       "        \"intensity\": 0.8,\n",
       "        \"context\": \"Can ingest up to 12 months of historical data from HubSpot and ad platforms; prospect excited it allows validating anecdotal podcast attribution before changing budget\",\n",
       "        \"source_utterance_indices\": [\n",
       "          19,\n",
       "          20,\n",
       "          21\n",
       "        ]\n",
       "      },\n",
       "      {\n",
       "        \"aspect\": \"overall product impression\",\n",
       "        \"sentiment\": \"positive\",\n",
       "        \"intensity\": 0.85,\n",
       "        \"context\": \"Prospect said 'I'm pretty sold on this honestly' and plans to recommend moving forward to CEO\",\n",
       "        \"source_utterance_indices\": [\n",
       "          21,\n",
       "          23,\n",
       "          25\n",
       "        ]\n",
       "      },\n",
       "      {\n",
       "        \"aspect\": \"internal buying process\",\n",
       "        \"sentiment\": \"positive\",\n",
       "        \"intensity\": 0.75,\n",
       "        \"context\": \"CEO trusts prospect's judgment on data tooling; expects straightforward approval; aims to have answer by Friday\",\n",
       "        \"source_utterance_indices\": [\n",
       "          23,\n",
       "          25\n",
       "        ]\n",
       "      }\n",
       "    ],\n",
       "    \"topics\": [\n",
       "      {\n",
       "        \"name\": \"Multi-touch attribution challenges\",\n",
       "        \"timeline_position\": \"early\",\n",
       "        \"relevance\": 1.0\n",
       "      },\n",
       "      {\n",
       "        \"name\": \"Manual reporting inefficiency (spreadsheet-based process)\",\n",
       "        \"timeline_position\": \"early\",\n",
       "        \"relevance\": 0.85\n",
       "      },\n",
       "      {\n",
       "        \"name\": \"Dark funnel / offline-to-online attribution\",\n",
       "        \"timeline_position\": \"early\",\n",
       "        \"relevance\": 0.95\n",
       "      },\n",
       "      {\n",
       "        \"name\": \"Podcast sponsorship ROI and spend justification\",\n",
       "        \"timeline_position\": \"early\",\n",
       "        \"relevance\": 0.9\n",
       "      },\n",
       "      {\n",
       "        \"name\": \"Self-reported attribution methodology (NLP-based)\",\n",
       "        \"timeline_position\": \"mid\",\n",
       "        \"relevance\": 0.8\n",
       "      },\n",
       "      {\n",
       "        \"name\": \"Pricing and ROI calculation\",\n",
       "        \"timeline_position\": \"mid\",\n",
       "        \"relevance\": 0.85\n",
       "      },\n",
       "      {\n",
       "        \"name\": \"Channel allocation strategy and confidence\",\n",
       "        \"timeline_position\": \"mid\",\n",
       "        \"relevance\": 0.8\n",
       "      },\n",
       "      {\n",
       "        \"name\": \"Technical integration and implementation\",\n",
       "        \"timeline_position\": \"mid\",\n",
       "        \"relevance\": 0.8\n",
       "      },\n",
       "      {\n",
       "        \"name\": \"Engineering resource constraints\",\n",
       "        \"timeline_position\": \"mid\",\n",
       "        \"relevance\": 0.7\n",
       "      },\n",
       "      {\n",
       "        \"name\": \"Historical data ingestion and retroactive modeling\",\n",
       "        \"timeline_position\": \"mid\",\n",
       "        \"relevance\": 0.75\n",
       "      },\n",
       "      {\n",
       "        \"name\": \"Next steps and procurement / internal approval\",\n",
       "        \"timeline_position\": \"late\",\n",
       "        \"relevance\": 0.85\n",
       "      },\n",
       "      {\n",
       "        \"name\": \"Case studies and social proof\",\n",
       "        \"timeline_position\": \"late\",\n",
       "        \"relevance\": 0.6\n",
       "      }\n",
       "    ],\n",
       "    \"entities\": [\n",
       "      {\n",
       "        \"name\": \"Nina\",\n",
       "        \"entity_type\": \"person\",\n",
       "        \"role\": \"Prospect, Head of Data at analytics consulting firm\",\n",
       "        \"mention_count\": 2\n",
       "      },\n",
       "      {\n",
       "        \"name\": \"Alisha\",\n",
       "        \"entity_type\": \"person\",\n",
       "        \"role\": \"CEO of prospect's company; final decision-maker\",\n",
       "        \"mention_count\": 2\n",
       "      },\n",
       "      {\n",
       "        \"name\": \"HubSpot\",\n",
       "        \"entity_type\": \"product\",\n",
       "        \"role\": \"Prospect's CRM platform; integration target\",\n",
       "        \"mention_count\": 4\n",
       "      },\n",
       "      {\n",
       "        \"name\": \"Google Ads\",\n",
       "        \"entity_type\": \"product\",\n",
       "        \"role\": \"Prospect's advertising platform; integration target\",\n",
       "        \"mention_count\": 2\n",
       "      },\n",
       "      {\n",
       "        \"name\": \"LinkedIn\",\n",
       "        \"entity_type\": \"company\",\n",
       "        \"role\": \"Prospect's advertising/campaign channel\",\n",
       "        \"mention_count\": 2\n",
       "      },\n",
       "      {\n",
       "        \"name\": \"Google\",\n",
       "        \"entity_type\": \"company\",\n",
       "        \"role\": \"Search engine referenced in attribution journey context\",\n",
       "        \"mention_count\": 1\n",
       "      }\n",
       "    ],\n",
       "    \"key_phrases\": [\n",
       "      {\n",
       "        \"phrase\": \"multi-touch attribution gaps\",\n",
       "        \"relevance\": 1.0,\n",
       "        \"context\": \"Core pain point that brought prospect to the conversation; downloaded benchmarking report on this topic\"\n",
       "      },\n",
       "      {\n",
       "        \"phrase\": \"dark funnel\",\n",
       "        \"relevance\": 0.95,\n",
       "        \"context\": \"Rep's characterization of the offline-to-online tracking problem with podcasts\"\n",
       "      },\n",
       "      {\n",
       "        \"phrase\": \"podcast-to-website-to-demo-request journey\",\n",
       "        \"relevance\": 0.9,\n",
       "        \"context\": \"Specific attribution gap prospect cannot track; listeners Google the company later and get misattributed\"\n",
       "      },\n",
       "      {\n",
       "        \"phrase\": \"180K annual podcast spend\",\n",
       "        \"relevance\": 0.9,\n",
       "        \"context\": \"Major budget decision at stake; CEO asking whether to double down or kill sponsorships\"\n",
       "      },\n",
       "      {\n",
       "        \"phrase\": \"self-reported attribution module\",\n",
       "        \"relevance\": 0.85,\n",
       "        \"context\": \"Product feature using free-text NLP to capture how-did-you-hear-about-us data\"\n",
       "      },\n",
       "      {\n",
       "        \"phrase\": \"two days a month\",\n",
       "        \"relevance\": 0.8,\n",
       "        \"context\": \"Analyst time spent on manual attribution reporting; quantified pain point\"\n",
       "      },\n",
       "      {\n",
       "        \"phrase\": \"42K annually\",\n",
       "        \"relevance\": 0.85,\n",
       "        \"context\": \"Starter tier pricing for prospect's company size and channel mix\"\n",
       "      },\n",
       "      {\n",
       "        \"phrase\": \"channel allocation decisions\",\n",
       "        \"relevance\": 0.8,\n",
       "        \"context\": \"Primary strategic value proposition; confidence in where to invest marketing budget\"\n",
       "      },\n",
       "      {\n",
       "        \"phrase\": \"one-click OAuth connection\",\n",
       "        \"relevance\": 0.65,\n",
       "        \"context\": \"HubSpot integration simplicity; no engineering needed\"\n",
       "      },\n",
       "      {\n",
       "        \"phrase\": \"12 months of historical data\",\n",
       "        \"relevance\": 0.75,\n",
       "        \"context\": \"Retroactive data ingestion capability that excited prospect for validation purposes\"\n",
       "      },\n",
       "      {\n",
       "        \"phrase\": \"three to four weeks implementation\",\n",
       "        \"relevance\": 0.7,\n",
       "        \"context\": \"Go-live timeline perceived favorably by prospect\"\n",
       "      },\n",
       "      {\n",
       "        \"phrase\": \"no spare engineering cycles\",\n",
       "        \"relevance\": 0.7,\n",
       "        \"context\": \"Key constraint for prospect; implementation must be handled by marketing ops\"\n",
       "      },\n",
       "      {\n",
       "        \"phrase\": \"attribution benchmarking report\",\n",
       "        \"relevance\": 0.6,\n",
       "        \"context\": \"Content asset that initiated the sales conversation\"\n",
       "      },\n",
       "      {\n",
       "        \"phrase\": \"cobbler's children have no shoes\",\n",
       "        \"relevance\": 0.4,\n",
       "        \"context\": \"Rep's rapport-building acknowledgment that analytics firms often have poor internal attribution\"\n",
       "      },\n",
       "      {\n",
       "        \"phrase\": \"answer by Friday\",\n",
       "        \"relevance\": 0.8,\n",
       "        \"context\": \"Prospect's commitment to a decision timeline; strong buying signal\"\n",
       "      }\n",
       "    ]\n",
       "  },\n",
       "  \"behavioral\": {\n",
       "    \"objection_triples\": [\n",
       "      {\n",
       "        \"objection\": {\n",
       "          \"type\": \"risk\",\n",
       "          \"specific_language\": \"We've tried the how-did-you-hear-about-us thing manually. The data quality was terrible. People just pick the first option.\",\n",
       "          \"speaker_role\": \"prospect_head_of_data\",\n",
       "          \"conversation_stage\": \"mid\",\n",
       "          \"source_utterance_indices\": [\n",
       "            9\n",
       "          ]\n",
       "        },\n",
       "        \"resolution\": {\n",
       "          \"type\": \"technical_demo\",\n",
       "          \"specific_language\": \"The trick is in how you present it and how you weight it. We use a free-text field instead of a dropdown, then use NLP to categorize and cross-reference with digital touchpoint data. Clients typically see 70 to 80% match rates between self-reported and digital signals.\",\n",
       "          \"speaker_role\": \"rep\",\n",
       "          \"source_utterance_indices\": [\n",
       "            10\n",
       "          ]\n",
       "        },\n",
       "        \"outcome\": {\n",
       "          \"resolved\": true,\n",
       "          \"deal_progressed\": true,\n",
       "          \"next_action\": \"Prospect acknowledged approach as clever and moved to pricing question\"\n",
       "        },\n",
       "        \"confidence\": 0.95\n",
       "      },\n",
       "      {\n",
       "        \"objection\": {\n",
       "          \"type\": \"pricing\",\n",
       "          \"specific_language\": \"42K. That's not nothing, but if it saves my analyst two days a month and helps us make a smarter call on 180K in podcast spend, the math works pretty quickly.\",\n",
       "          \"speaker_role\": \"prospect_head_of_data\",\n",
       "          \"conversation_stage\": \"mid\",\n",
       "          \"source_utterance_indices\": [\n",
       "            13\n",
       "          ]\n",
       "        },\n",
       "        \"resolution\": {\n",
       "          \"type\": \"roi_argument\",\n",
       "          \"specific_language\": \"Exactly. And that analyst time is probably the smaller piece. The real value is having confidence in your channel allocation decisions. If podcasts are actually driving 30% of your pipeline but your current data says 5%, that changes your entire strategy.\",\n",
       "          \"speaker_role\": \"rep\",\n",
       "          \"source_utterance_indices\": [\n",
       "            14\n",
       "          ]\n",
       "        },\n",
       "        \"outcome\": {\n",
       "          \"resolved\": true,\n",
       "          \"deal_progressed\": true,\n",
       "          \"next_action\": \"Prospect moved to implementation questions\"\n",
       "        },\n",
       "        \"confidence\": 0.85\n",
       "      },\n",
       "      {\n",
       "        \"objection\": {\n",
       "          \"type\": \"implementation\",\n",
       "          \"specific_language\": \"That's really important to us. We don't have spare engineering cycles.\",\n",
       "          \"speaker_role\": \"prospect_head_of_data\",\n",
       "          \"conversation_stage\": \"mid\",\n",
       "          \"source_utterance_indices\": [\n",
       "            19\n",
       "          ]\n",
       "        },\n",
       "        \"resolution\": {\n",
       "          \"type\": \"risk_mitigation\",\n",
       "          \"specific_language\": \"Marketing ops can handle the whole thing. The HubSpot integration is a one-click OAuth connection. We handle the data pipeline setup on our end.\",\n",
       "          \"speaker_role\": \"rep\",\n",
       "          \"source_utterance_indices\": [\n",
       "            18\n",
       "          ]\n",
       "        },\n",
       "        \"outcome\": {\n",
       "          \"resolved\": true,\n",
       "          \"deal_progressed\": true,\n",
       "          \"next_action\": \"Prospect moved to historical data question\"\n",
       "        },\n",
       "        \"confidence\": 0.92\n",
       "      },\n",
       "      {\n",
       "        \"objection\": {\n",
       "          \"type\": \"authority\",\n",
       "          \"specific_language\": \"I need to loop in our CEO, Alisha, but I'm going to recommend we do this.\",\n",
       "          \"speaker_role\": \"prospect_head_of_data\",\n",
       "          \"conversation_stage\": \"late\",\n",
       "          \"source_utterance_indices\": [\n",
       "            23\n",
       "          ]\n",
       "        },\n",
       "        \"resolution\": {\n",
       "          \"type\": \"social_proof\",\n",
       "          \"specific_language\": \"Absolutely. I have two great case studies from similar-sized consulting firms. I'll package those with the proposal. Would it be helpful to have a short call with Alisha as well, or do you think you can handle it internally?\",\n",
       "          \"speaker_role\": \"rep\",\n",
       "          \"source_utterance_indices\": [\n",
       "            24\n",
       "          ]\n",
       "        },\n",
       "        \"outcome\": {\n",
       "          \"resolved\": true,\n",
       "          \"deal_progressed\": true,\n",
       "          \"next_action\": \"Prospect will present proposal and case studies to CEO Alisha, aims for answer by Friday\"\n",
       "        },\n",
       "        \"confidence\": 0.9\n",
       "      }\n",
       "    ],\n",
       "    \"buying_intent_markers\": [\n",
       "      {\n",
       "        \"type\": \"budget_confirmation\",\n",
       "        \"evidence\": \"Okay, what does something like this cost for a company our size? We're not a big budget operation.\",\n",
       "        \"confidence\": 0.8,\n",
       "        \"source_utterance_indices\": [\n",
       "          11\n",
       "        ]\n",
       "      },\n",
       "      {\n",
       "        \"type\": \"if_to_when_shift\",\n",
       "        \"evidence\": \"42K. That's not nothing, but if it saves my analyst two days a month and helps us make a smarter call on 180K in podcast spend, the math works pretty quickly.\",\n",
       "        \"confidence\": 0.85,\n",
       "        \"source_utterance_indices\": [\n",
       "          13\n",
       "        ]\n",
       "      },\n",
       "      {\n",
       "        \"type\": \"timeline_question\",\n",
       "        \"evidence\": \"Right. Okay, what does implementation look like? We run HubSpot and Google Ads primarily.\",\n",
       "        \"confidence\": 0.9,\n",
       "        \"source_utterance_indices\": [\n",
       "          15\n",
       "        ]\n",
       "      },\n",
       "      {\n",
       "        \"type\": \"implementation_detail\",\n",
       "        \"evidence\": \"Three to four weeks is fast. Do we need engineering resources or can my marketing ops person handle it?\",\n",
       "        \"confidence\": 0.95,\n",
       "        \"source_utterance_indices\": [\n",
       "          17\n",
       "        ]\n",
       "      },\n",
       "      {\n",
       "        \"type\": \"implementation_detail\",\n",
       "        \"evidence\": \"What about data we've already collected? Can we do any historical attribution or is it forward-looking only?\",\n",
       "        \"confidence\": 0.92,\n",
       "        \"source_utterance_indices\": [\n",
       "          19\n",
       "        ]\n",
       "      },\n",
       "      {\n",
       "        \"type\": \"if_to_when_shift\",\n",
       "        \"evidence\": \"Oh that's great. That would let us test the accuracy before we commit to changing our budget allocation. I'm pretty sold on this honestly.\",\n",
       "        \"confidence\": 0.95,\n",
       "        \"source_utterance_indices\": [\n",
       "          21\n",
       "        ]\n",
       "      },\n",
       "      {\n",
       "        \"type\": \"next_steps_request\",\n",
       "        \"evidence\": \"What would next steps look like if we wanted to move forward?\",\n",
       "        \"confidence\": 0.99,\n",
       "        \"source_utterance_indices\": [\n",
       "          21\n",
       "        ]\n",
       "      },\n",
       "      {\n",
       "        \"type\": \"stakeholder_introduction\",\n",
       "        \"evidence\": \"I need to loop in our CEO, Alisha, but I'm going to recommend we do this.\",\n",
       "        \"confidence\": 0.95,\n",
       "        \"source_utterance_indices\": [\n",
       "          23\n",
       "        ]\n",
       "      },\n",
       "      {\n",
       "        \"type\": \"budget_confirmation\",\n",
       "        \"evidence\": \"She trusts my judgment on data tooling. If she has questions I'll pull you in, but honestly this should be straightforward. Send me that proposal and I'll aim to have an answer by Friday.\",\n",
       "        \"confidence\": 0.9,\n",
       "        \"source_utterance_indices\": [\n",
       "          25\n",
       "        ]\n",
       "      }\n",
       "    ],\n",
       "    \"competitive_mentions\": [],\n",
       "    \"engagement_trajectory\": [\n",
       "      {\n",
       "        \"phase\": \"early\",\n",
       "        \"participation_level\": \"high\",\n",
       "        \"question_depth\": \"deep\",\n",
       "        \"energy\": \"high\",\n",
       "        \"notes\": \"Prospect immediately self-identified a specific pain point (multi-touch attribution gaps), volunteered detailed context about team size, channels, manual processes, and time cost. Showed vulnerability by calling their own attribution 'embarrassingly bad.' Very forthcoming with specifics like analyst time (two days/month) and distrust of numbers.\"\n",
       "      },\n",
       "      {\n",
       "        \"phase\": \"mid\",\n",
       "        \"participation_level\": \"high\",\n",
       "        \"question_depth\": \"deep\",\n",
       "        \"energy\": \"high\",\n",
       "        \"notes\": \"Prospect pushed back constructively on self-reported attribution (showing sophistication), proactively asked about pricing, and self-justified the ROI ('the math works pretty quickly'). Transitioned rapidly from pricing to implementation questions, showing accelerating momentum. Asked about engineering resource requirements and historical data ingestion — both deep, practical questions indicating serious evaluation.\"\n",
       "      },\n",
       "      {\n",
       "        \"phase\": \"late\",\n",
       "        \"participation_level\": \"high\",\n",
       "        \"question_depth\": \"moderate\",\n",
       "        \"energy\": \"high\",\n",
       "        \"notes\": \"Prospect explicitly stated 'I'm pretty sold on this honestly,' asked for next steps unprompted, volunteered a timeline commitment ('aim to have an answer by Friday'), and expressed confidence in internal approval ('She trusts my judgment on data tooling'). Energy remained high through close with humor ('you and me both'). Champion behavior is clear — prospect is self-qualifying and preparing to sell internally.\"\n",
       "      }\n",
       "    ]\n",
       "  },\n",
       "  \"psychographic\": {\n",
       "    \"mental_model\": {\n",
       "      \"primary\": \"efficiency\",\n",
       "      \"secondary\": \"risk_mitigation\",\n",
       "      \"evidence\": [\n",
       "        \"My analyst spends probably two days a month just building the attribution report, and honestly, I don't trust the numbers.\",\n",
       "        \"42K. That's not nothing, but if it saves my analyst two days a month and helps us make a smarter call on 180K in podcast spend, the math works pretty quickly.\",\n",
       "        \"That's really important to us. We don't have spare engineering cycles.\",\n",
       "        \"That would let us test the accuracy before we commit to changing our budget allocation.\",\n",
       "        \"And our CEO keeps asking me whether we should double down or kill them. I can't give her a straight answer with the data I have.\"\n",
       "      ],\n",
       "      \"confidence\": 0.9,\n",
       "      \"reasoning\": \"Nina's primary concern is efficiency: she repeatedly highlights the time her analyst wastes on manual spreadsheet work (two days a month), the lack of engineering cycles, and the need for a tool that marketing ops can handle without engineering involvement. She frames the ROI calculation around time savings and better decision-making speed. Her secondary model is risk_mitigation — she is deeply uncomfortable making a $180K budget decision without trustworthy data. She doesn't frame the problem as 'grow revenue' but rather as 'avoid making the wrong call.' She wants to validate attribution accuracy with historical data before committing to budget changes ('test the accuracy before we commit to changing our budget allocation'). She's not trying to expand capabilities or grow into new markets; she's trying to stop flying blind and reduce the risk of misallocating spend. While there's an element of cost_reduction (the $180K podcast decision), she frames it as needing confidence and accuracy rather than purely cutting costs — it's about making the right decision, not necessarily spending less.\"\n",
       "    },\n",
       "    \"persona_indicators\": [\n",
       "      {\n",
       "        \"archetype\": \"analytical_evaluator\",\n",
       "        \"confidence\": 0.85,\n",
       "        \"evidence\": [\n",
       "          \"We've tried the how-did-you-hear-about-us thing manually. The data quality was terrible. People just pick the first option.\",\n",
       "          \"What does implementation look like? We run HubSpot and Google Ads primarily.\",\n",
       "          \"Do we need engineering resources or can my marketing ops person handle it?\",\n",
       "          \"Can we do any historical attribution or is it forward-looking only?\",\n",
       "          \"That would let us test the accuracy before we commit to changing our budget allocation.\",\n",
       "          \"42K. That's not nothing, but if it saves my analyst two days a month and helps us make a smarter call on 180K in podcast spend, the math works pretty quickly.\"\n",
       "        ],\n",
       "        \"reasoning\": \"Nina asks detailed, specific technical questions in a methodical sequence: cost, implementation timeline, engineering requirements, historical data capabilities, and integration specifics. She pushes back on the self-reported attribution approach based on prior experience with data quality, showing she evaluates claims critically rather than accepting them at face value. She does mental ROI math on the spot (42K vs. 2 days/month + 180K decision). She wants to validate the tool's accuracy against historical data before making strategic changes — a classic analytical evaluator behavior.\"\n",
       "      },\n",
       "      {\n",
       "        \"archetype\": \"executive_champion\",\n",
       "        \"confidence\": 0.45,\n",
       "        \"evidence\": [\n",
       "          \"I'm going to recommend we do this.\",\n",
       "          \"She trusts my judgment on data tooling. If she has questions I'll pull you in, but honestly this should be straightforward.\",\n",
       "          \"I think I can handle it.\"\n",
       "        ],\n",
       "        \"reasoning\": \"Nina shows some executive champion traits: she's clearly the internal decision driver for data tooling and expresses confidence in her ability to champion this purchase to the CEO without needing the rep's direct involvement. She moves quickly toward a decision and sets a Friday deadline. However, her confidence score is moderate because she's more analytical than visionary — she's not focused on strategic narrative or peer social proof in an executive sense, but she does request case studies as ammunition, which is a champion behavior.\"\n",
       "      },\n",
       "      {\n",
       "        \"archetype\": \"reluctant_adopter\",\n",
       "        \"confidence\": 0.15,\n",
       "        \"evidence\": [\n",
       "          \"We're not a big budget operation.\",\n",
       "          \"42K. That's not nothing.\"\n",
       "        ],\n",
       "        \"reasoning\": \"There are faint signals of budget sensitivity, but Nina is far from reluctant. She proactively moves toward next steps, expresses enthusiasm ('I'm pretty sold on this honestly'), and her caution around budget is rational rather than anxiety-driven. She was not brought in by someone else — she initiated engagement by downloading the report. Very low confidence for this archetype.\"\n",
       "      }\n",
       "    ],\n",
       "    \"language_fingerprint\": {\n",
       "      \"distinctive_vocabulary\": [\n",
       "        \"dark funnel\",\n",
       "        \"stitching together\",\n",
       "        \"flying blind\",\n",
       "        \"spare engineering cycles\",\n",
       "        \"budget allocation\",\n",
       "        \"channel mix\",\n",
       "        \"the math works\",\n",
       "        \"data quality\"\n",
       "      ],\n",
       "      \"metaphors\": [\n",
       "        \"the cobbler's children have no shoes (acknowledged/adopted from rep)\",\n",
       "        \"flying blind (used implicitly via rep, accepted framing)\",\n",
       "        \"stitching everything together in spreadsheets\",\n",
       "        \"get you off those spreadsheets (echoed by rep, endorsed)\"\n",
       "      ],\n",
       "      \"framing_patterns\": [\n",
       "        \"If X saves Y and helps Z, the math works — ROI justification through concrete arithmetic\",\n",
       "        \"We've tried [approach] manually. [Negative outcome]. — Prior experience as credibility filter\",\n",
       "        \"That's really important to us. We don't have [resource]. — Constraint-first evaluation of feasibility\",\n",
       "        \"I can't give her a straight answer with the data I have. — Framing problems as inability to act due to data gaps\",\n",
       "        \"That would let us test [claim] before we commit to [action]. — Validation-before-commitment pattern\",\n",
       "        \"I'm pretty sold on this honestly. — Direct emotional signaling after analytical validation\"\n",
       "      ]\n",
       "    }\n",
       "  },\n",
       "  \"multimodal\": null,\n",
       "  \"overall_confidence\": 0.83,\n",
       "  \"notes\": [\n",
       "    \"No paralinguistic annotations — multimodal extraction skipped\"\n",
       "  ]\n",
       "}</pre>\n",
       "</details>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Collapsible raw JSON output\n",
    "raw_json = result.model_dump_json(indent=2)\n",
    "display(HTML(f\"\"\"\n",
    "<details>\n",
    "<summary><b>Raw JSON output</b> (click to expand)</summary>\n",
    "<pre>{raw_json}</pre>\n",
    "</details>\n",
    "\"\"\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Before & After: Input vs Extracted Features\n",
    "\n",
    "Side-by-side comparison of raw conversation moments and the structured signals extracted from them.\n",
    "This demonstrates the transformation from text to actionable intelligence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "### Example: Objection Detection"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**INPUT** (Raw Transcript)"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  [9] prospect_head_of_data: We've tried the how-did-you-hear-about-us thing manually. The data quality was terrible. People just pick the first option.\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "**OUTPUT** (Structured Extraction)"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "\n",
       "- **Type:** `risk`\n",
       "- **Stage:** `mid`\n",
       "- **Speaker:** `prospect_head_of_data`\n",
       "- **Language:** \"We've tried the how-did-you-hear-about-us thing manually. The data quality was terrible. People just pick the first option.\"\n",
       "- **Resolution:** technical_demo — \"The trick is in how you present it and how you weight it. We use a free-text field instead of a dropdown, then use NLP to categorize and cross-reference with digital touchpoint data. Clients typically see 70 to 80% match rates between self-reported and digital signals.\"\n",
       "- **Outcome:** Resolved, Deal progressed\n",
       "- **Confidence:** 0.95\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "### Example: Aspect-Based Sentiment"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**INPUT** (Raw Transcript)"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  [1] prospect_head_of_data: Yeah, the section on multi-touch attribution gaps was exactly what we're wrestling with. We're a 60-person shop doing analytics consulting, and our own marketing attribution is embarrassingly bad.\n",
      "\n",
      "  [3] prospect_head_of_data: We're running campaigns across LinkedIn, Google, our podcast sponsorships, and a bunch of partner co-marketing stuff. Right now we're stitching everything together in spreadsheets. My analyst spends probably two days a month just building the attribution report, and honestly, I don't trust the numbers.\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "**OUTPUT** (Structured Extraction)"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "\n",
       "- **Aspect:** `current attribution process`\n",
       "- **Sentiment:** `negative` (intensity: 0.85)\n",
       "- **Context:** \"Manual spreadsheet-based attribution taking two days per month with untrusted output; described as 'embarrassingly bad'\"\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Example: Objection Detection — raw utterances vs structured extraction\n",
    "if result.behavioral.objection_triples:\n",
    "    triple = result.behavioral.objection_triples[0]\n",
    "    obj = triple.objection\n",
    "\n",
    "    display(Markdown(\"### Example: Objection Detection\"))\n",
    "    display(Markdown(\"**INPUT** (Raw Transcript)\"))\n",
    "    for idx in obj.source_utterance_indices[:3]:\n",
    "        if idx < len(transcript.utterances):\n",
    "            u = transcript.utterances[idx]\n",
    "            print(f\"  [{u.turn_index}] {u.speaker}: {u.text}\\n\")\n",
    "\n",
    "    display(Markdown(\"**OUTPUT** (Structured Extraction)\"))\n",
    "    display(Markdown(f\"\"\"\n",
    "- **Type:** `{obj.type}`\n",
    "- **Stage:** `{obj.conversation_stage}`\n",
    "- **Speaker:** `{obj.speaker_role}`\n",
    "- **Language:** \"{obj.specific_language}\"\n",
    "- **Resolution:** {triple.resolution.type + ' — \"' + triple.resolution.specific_language + '\"' if triple.resolution else \"None\"}\n",
    "- **Outcome:** {\"Resolved\" if triple.outcome.resolved else \"Unresolved\"}, Deal {\"progressed\" if triple.outcome.deal_progressed else \"stalled\"}\n",
    "- **Confidence:** {triple.confidence:.2f}\n",
    "\"\"\"))\n",
    "\n",
    "# Example: Aspect Sentiment — pick a negative or mixed aspect if available\n",
    "interesting = next((a for a in result.surface.aspects if a.sentiment in (\"negative\", \"mixed\")), None)\n",
    "if not interesting and result.surface.aspects:\n",
    "    interesting = result.surface.aspects[0]\n",
    "\n",
    "if interesting:\n",
    "    display(Markdown(\"### Example: Aspect-Based Sentiment\"))\n",
    "    display(Markdown(\"**INPUT** (Raw Transcript)\"))\n",
    "    for idx in interesting.source_utterance_indices[:2]:\n",
    "        if idx < len(transcript.utterances):\n",
    "            u = transcript.utterances[idx]\n",
    "            print(f\"  [{u.turn_index}] {u.speaker}: {u.text}\\n\")\n",
    "\n",
    "    display(Markdown(\"**OUTPUT** (Structured Extraction)\"))\n",
    "    display(Markdown(f\"\"\"\n",
    "- **Aspect:** `{interesting.aspect}`\n",
    "- **Sentiment:** `{interesting.sentiment}` (intensity: {interesting.intensity:.2f})\n",
    "- **Context:** \"{interesting.context}\"\n",
    "\"\"\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Transcript Summary\n",
    "\n",
    "Generate a human-readable executive summary of the call. This is a separate extraction step\n",
    "(1 LLM call) that produces narrative prose, key moments, action items, and deal signals —\n",
    "designed for sales managers who need to understand a call without reading the full transcript."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summary generated for cloudfirst_analytics_call1\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "### Executive Summary"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "This call was between the sales rep and Nina, Head of Data at a 60-person analytics consulting firm, who had previously downloaded an attribution benchmarking report. The conversation centered on Nina's inability to accurately attribute marketing results across channels—particularly podcast sponsorships ($180K/year)—due to manual, spreadsheet-based processes that consume two analyst-days per month and produce untrustworthy data. The core pain point is a 'dark funnel' problem: prospects hear about the firm through podcasts but are misattributed to organic search, leaving the CEO without reliable data to decide whether to scale or cut podcast spend.\n",
       "\n",
       "Nina raised one notable objection around self-reported attribution, noting that a previous manual 'how did you hear about us' approach yielded poor data quality. The rep addressed this effectively by explaining their NLP-powered free-text field approach and its 70-80% match rate with digital signals, which satisfied Nina. She also asked about cost, implementation complexity, engineering resource requirements, and historical data ingestion—all of which the rep answered favorably. The $42K annual price point was received positively given the clear ROI against the $180K podcast spend decision and analyst time savings. Nina specifically valued that no engineering resources would be needed and that implementation could be handled by marketing ops alone.\n",
       "\n",
       "The deal is in strong shape. Nina expressed clear buying intent and stated she would recommend the purchase to her CEO, Alisha, who has final sign-off authority. The rep agreed to send a proposal with two consulting-firm case studies by end of day. Nina committed to providing an answer by Friday. The prospect declined to have the rep join a call with the CEO, indicating high internal confidence but also introducing a risk that the rep won't have direct access to the final decision-maker."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "### Key Moments"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**1. INSIGHT** (turns [5, 6, 7])"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Nina revealed that $180K in annual podcast spend is essentially unattributable, and her CEO is pressing for a scale-or-kill decision she cannot support with data.\n",
      "  Significance: This crystallized the core business pain and established a concrete, high-dollar ROI narrative that makes the $42K solution price easy to justify internally.\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "**2. OBJECTION** (turns [9, 10])"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Nina pushed back on self-reported attribution, stating a previous manual 'how did you hear about us' attempt produced terrible data quality because respondents just picked the first option.\n",
      "  Significance: This was the only real objection in the call. The rep's explanation of NLP-powered free-text categorization with 70-80% digital match rates directly addressed the concern and restored credibility in the approach.\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "**3. BREAKTHROUGH** (turns [19, 20, 21])"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  The rep explained that up to 12 months of historical data could be ingested from HubSpot and ad platforms, enabling retroactive attribution modeling.\n",
      "  Significance: This was a pivotal moment because it allows Nina to validate podcast attribution against anecdotal evidence before committing to budget reallocation—removing a key adoption risk and accelerating her confidence.\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "**4. COMMITMENT** (turns [21, 23, 25])"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Nina said she was 'pretty sold,' committed to recommending the purchase to her CEO, and set a Friday deadline for a decision.\n",
      "  Significance: This represents a strong verbal commitment with a specific timeline, moving the deal to a clear proposal stage with an identified close date.\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "**5. RISK** (turns [24, 25])"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Nina declined to have the rep join a call with CEO Alisha, preferring to handle the internal conversation herself.\n",
      "  Significance: While Nina expressed high confidence, the rep will have no direct access to the final decision-maker. If Alisha raises objections or has questions Nina can't fully address, the deal could stall without the rep's involvement.\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "### Action Items"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "| Action | Owner | Criticality |\n",
       "| --- | --- | --- |\n",
       "| Send proposal with $42K annual pricing, quarterly billing terms, and standard annual agreement details | rep | high |\n",
       "| Include two case studies from similar-sized analytics/consulting firms with the proposal | rep | high |\n",
       "| Present the proposal and case studies to CEO Alisha and secure approval | Nina | high |\n",
       "| Provide a decision by Friday | Nina | high |\n",
       "| If signed this week, schedule implementation kickoff for next Monday with target go-live by end of March | rep | medium |\n",
       "| Be available for a follow-up call with CEO Alisha if questions arise | rep | medium |"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "### Prospect Priorities"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  1. Accurate attribution of podcast sponsorships ($180K/year) to support a strategic scale-or-cut budget decision for the CEO\n",
      "  2. Eliminating manual, spreadsheet-based attribution processes that consume two analyst-days per month and produce untrustworthy data\n",
      "  3. Low implementation burden—no engineering resources required, marketing ops can handle setup independently\n",
      "  4. Ability to validate the solution with historical data before making major budget reallocation decisions\n",
      "  5. Data quality and trustworthiness of self-reported attribution methods\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "### Concerns to Address"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  1. CEO Alisha is the final decision-maker and the rep has no direct access to her; if Nina's internal pitch encounters resistance, there is no fallback engagement plan\n",
      "  2. Nina described the company as 'not a big budget operation'—even though she rationalized the ROI, price sensitivity could resurface during CEO review or if competing budget priorities arise\n",
      "  3. The prospect's past negative experience with self-reported attribution may linger as a credibility concern for the CEO who wasn't on this call to hear the NLP explanation\n",
      "  4. No discussion of contract flexibility (e.g., month-to-month, exit clauses, or pilot options) which could become a sticking point for a budget-conscious 60-person firm committing to $42K annually\n",
      "  5. Podcast sponsorship tracking via partner co-marketing channels was mentioned but not deeply explored—if the solution doesn't fully cover all co-marketing attribution, that gap could surface post-sale\n",
      "CPU times: user 24.4 ms, sys: 4.6 ms, total: 29 ms\n",
      "Wall time: 35.3 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "summary = extract_summary(transcript, client=client)\n",
    "print(f\"Summary generated for {SELECTED_CALL_ID}\")\n",
    "display_summary(summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Layer-by-Layer Exploration\n",
    "\n",
    "Run individual extraction layers to inspect intermediate outputs.\n",
    "This is useful for:\n",
    "- Debugging a specific layer's behavior\n",
    "- Understanding how each prompt template processes the transcript\n",
    "- Timing individual layers to identify bottlenecks\n",
    "- Inspecting raw LLM output before schema validation/coercion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Formatted transcript length: 5666 chars\n",
      "\n",
      "--- First 500 chars ---\n",
      "[0] rep: Hi Nina, thanks for hopping on. I saw you downloaded our attribution benchmarking report last week. What caught your eye?\n",
      "[1] prospect_head_of_data: Yeah, the section on multi-touch attribution gaps was exactly what we're wrestling with. We're a 60-person shop doing analytics consulting, and our own marketing attribution is embarrassingly bad.\n",
      "[2] rep: Ha, the cobbler's children have no shoes. That's actually more common than you'd think with analytics firms. Tell me more about what you\n"
     ]
    }
   ],
   "source": [
    "transcript_text = _format_transcript(transcript)\n",
    "print(f\"Formatted transcript length: {len(transcript_text)} chars\")\n",
    "print(f\"\\n--- First 500 chars ---\")\n",
    "print(transcript_text[:500])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "### Layer 1: Surface Signals"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "### Aspect Sentiments"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "| Aspect | Sentiment | Intensity | Context |\n",
       "| --- | --- | --- | --- |\n",
       "| current attribution process | negative | 0.85 | Manual spreadsheet-based attribution takes two days/month, produces untrustworth |\n",
       "| pricing | mixed | 0.60 | Prospect acknowledges 42K is 'not nothing' but quickly rationalizes ROI against  |\n",
       "| self-reported attribution module | positive | 0.70 | Initially skeptical due to past poor experience with manual how-did-you-hear fie |\n",
       "| integration / implementation | positive | 0.85 | Native HubSpot integration, plug-and-play Google Ads, 3-4 week timeline, no engi |\n",
       "| historical data ingestion | positive | 0.80 | Ability to ingest 12 months of historical data from HubSpot and ad platforms exc |\n",
       "| ROI / value proposition | positive | 0.85 | Prospect sees clear ROI: saving analyst time, gaining confidence on 180K podcast |\n",
       "| ease of setup / resource requirements | positive | 0.90 | Marketing ops can handle entire implementation without engineering resources; on |\n",
       "| podcast attribution / dark funnel | negative | 0.80 | Current inability to track podcast-to-demo journey means 180K annual spend decis |\n",
       "| internal buying process | positive | 0.75 | Prospect is confident she can handle internal approval; CEO trusts her judgment  |"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "### Topics"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "| Topic | Position | Relevance |\n",
       "| --- | --- | --- |\n",
       "| Multi-touch attribution gaps and challenges | early | 1.00 |\n",
       "| Dark funnel / offline-to-online tracking (podcast attribution) | early | 0.95 |\n",
       "| Manual reporting and analyst time costs | early | 0.80 |\n",
       "| Self-reported attribution methodology (NLP free-text approach) | mid | 0.85 |\n",
       "| Pricing and budget justification | mid | 0.80 |\n",
       "| Channel budget allocation decisions (podcast spend) | mid | 0.85 |\n",
       "| Technical implementation and integrations | mid | 0.80 |\n",
       "| Historical data ingestion and retroactive validation | late | 0.75 |\n",
       "| Next steps / proposal and internal approval process | late | 0.70 |"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "### Named Entities"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "| Name | Type | Role | Mentions |\n",
       "| --- | --- | --- | --- |\n",
       "| Nina | person | Prospect, Head of Data at analytics consulting firm | 2 |\n",
       "| Alisha | person | CEO of prospect's company, final decision-maker | 2 |\n",
       "| HubSpot | product | Prospect's CRM / marketing platform | 4 |\n",
       "| Google Ads | product | Prospect's advertising platform | 2 |\n",
       "| LinkedIn | company | Prospect's advertising / campaign channel | 2 |"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "### Key Phrases"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "| Phrase | Relevance | Context |\n",
       "| --- | --- | --- |\n",
       "| multi-touch attribution gaps | 1.00 | Core pain point that initiated the conversation; prospect do |\n",
       "| dark funnel | 0.95 | Rep's framing of the podcast-to-online tracking challenge; r |\n",
       "| podcast-to-website-to-demo-request journey | 0.90 | Specific attribution gap prospect cannot track; drives the 1 |\n",
       "| self-reported attribution module | 0.85 | Key product feature addressing the dark funnel problem using |\n",
       "| 180K annual podcast spend | 0.90 | Major budget decision flying blind; CEO asking whether to do |\n",
       "| 42K annually | 0.85 | Starter tier pricing for prospect's company size and channel |\n",
       "| two days a month manual reporting | 0.75 | Analyst time cost of current spreadsheet-based attribution p |\n",
       "| 70 to 80% match rates | 0.80 | Claimed accuracy of NLP-based self-reported attribution cros |\n",
       "| no engineering resources needed | 0.80 | Critical requirement for prospect who has no spare engineeri |\n",
       "| 12 months historical data ingestion | 0.75 | Retroactive attribution capability that allows prospect to v |\n",
       "| three to four weeks implementation | 0.70 | Fast timeline perceived positively; live by end of March if  |\n",
       "| channel allocation decisions | 0.85 | Strategic value of the product — confidence in how marketing |\n",
       "| cobbler's children have no shoes | 0.50 | Rep's rapport-building metaphor acknowledging irony of analy |"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 17 ms, sys: 3.74 ms, total: 20.7 ms\n",
      "Wall time: 31.2 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "display(Markdown(\"### Layer 1: Surface Signals\"))\n",
    "surface_data = _extract_layer(client, SURFACE_EXTRACTION_PROMPT, transcript_text)\n",
    "surface = SurfaceSignals.model_validate(_coerce_to_schema(surface_data, SurfaceSignals))\n",
    "display_surface(surface)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "display(Markdown(\"### Layer 2: Behavioral Signals\"))\n",
    "behavioral_data = _extract_layer(client, BEHAVIORAL_EXTRACTION_PROMPT, transcript_text)\n",
    "behavioral = BehavioralSignals.model_validate(_coerce_to_schema(behavioral_data, BehavioralSignals))\n",
    "display_behavioral(behavioral)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "display(Markdown(\"### Layer 3: Psychographic Signals\"))\n",
    "psychographic_data = _extract_layer(client, PSYCHOGRAPHIC_EXTRACTION_PROMPT, transcript_text)\n",
    "psychographic = PsychographicSignals.model_validate(_coerce_to_schema(psychographic_data, PsychographicSignals))\n",
    "display_psychographic(psychographic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "display(Markdown(\"### Layer 4: Multimodal Divergence\"))\n",
    "if _has_paralinguistic(transcript):\n",
    "    multimodal_data = _extract_layer(client, MULTIMODAL_DIVERGENCE_PROMPT, transcript_text)\n",
    "    multimodal = MultimodalSignals.model_validate(_coerce_to_schema(multimodal_data, MultimodalSignals))\n",
    "    display_multimodal(multimodal)\n",
    "else:\n",
    "    print(\"Skipped — no paralinguistic annotations in this transcript.\")\n",
    "    print(\"Select a transcript with paralinguistic data (e.g., techcorp_call1, safeguard_inc_call1)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspect the raw dict returned by the LLM before Pydantic validation.\n",
    "# Useful for debugging schema coercion issues.\n",
    "# Change this to: surface_data, behavioral_data, psychographic_data, or multimodal_data\n",
    "display(Markdown(\"### Raw Layer Output (pre-validation)\"))\n",
    "pretty_json(surface_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Prompt Tuning Workbench\n",
    "\n",
    "Load, edit, and test prompt modifications without touching the prompt files on disk.\n",
    "\n",
    "**Workflow:**\n",
    "1. Select a layer and view its current prompt template\n",
    "2. Copy and modify the prompt in the editable cell below\n",
    "3. Run extraction with the modified prompt\n",
    "4. Compare results against the original prompt\n",
    "\n",
    "Prompts use `{transcript}` as the placeholder for the formatted transcript text.\n",
    "Double braces `{{` and `}}` are used for literal braces in JSON schema examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt: surface\n",
      "Length: 1397 chars\n",
      "\n",
      "--- Full prompt template ---\n",
      "You are extracting Layer 1 (Surface) signals from a sales call transcript.\n",
      "\n",
      "TRANSCRIPT:\n",
      "{transcript}\n",
      "\n",
      "Extract these signal types:\n",
      "\n",
      "1. ASPECT-BASED SENTIMENT: For each distinct aspect discussed (e.g., \"pricing\", \"product\",\n",
      "   \"integration\", \"support\"), identify the sentiment, intensity (0-1), context explaining\n",
      "   the sentiment, and which utterance indices support it. A single utterance can contain\n",
      "   multiple aspects with different sentiments.\n",
      "\n",
      "2. TOPIC DETECTION: What subjects are discussed? Position each in the conversation\n",
      "   timeline (early/mid/late) and rate relevance (0-1).\n",
      "\n",
      "3. NAMED ENTITIES: People, companies, products, and competitors mentioned. Include\n",
      "   entity type, role if inferable, and mention count.\n",
      "\n",
      "4. KEY PHRASES: Important terms and concepts, weighted by relevance to the sales context.\n",
      "\n",
      "Return ONLY valid JSON:\n",
      "{{\n",
      "  \"aspects\": [\n",
      "    {{\"aspect\": \"string\", \"sentiment\": \"positive|negative|neutral|mixed\",\n",
      "      \"intensity\": 0.0-1.0, \"context\": \"string or null\",\n",
      "      \"source_utterance_indices\": [int]}}\n",
      "  ],\n",
      "  \"topics\": [\n",
      "    {{\"name\": \"string\", \"timeline_position\": \"early|mid|late\", \"relevance\": 0.0-1.0}}\n",
      "  ],\n",
      "  \"entities\": [\n",
      "    {{\"name\": \"string\", \"entity_type\": \"person|company|product|competitor\",\n",
      "      \"role\": \"string or null\", \"mention_count\": int}}\n",
      "  ],\n",
      "  \"key_phrases\": [\n",
      "    {{\"phrase\": \"string\", \"relevance\": 0.0-1.0, \"context\": \"string or null\"}}\n",
      "  ]\n",
      "}}\n"
     ]
    }
   ],
   "source": [
    "# Available: \"surface\", \"behavioral\", \"psychographic\", \"multimodal_divergence\"\n",
    "PROMPT_NAME = \"surface\"\n",
    "\n",
    "current_prompt = load_prompt(PROMPT_NAME)\n",
    "print(f\"Prompt: {PROMPT_NAME}\")\n",
    "print(f\"Length: {len(current_prompt)} chars\")\n",
    "print(f\"\\n--- Full prompt template ---\")\n",
    "print(current_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# EDIT THIS PROMPT to test modifications.\n",
    "# The {transcript} placeholder will be filled automatically.\n",
    "# Use {{ and }} for literal braces in JSON examples.\n",
    "# ============================================================\n",
    "\n",
    "modified_prompt = current_prompt  # Start from the current prompt and modify as needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# Map prompt name to schema class for validation\n",
    "SCHEMA_MAP = {\n",
    "    \"surface\": SurfaceSignals,\n",
    "    \"behavioral\": BehavioralSignals,\n",
    "    \"psychographic\": PsychographicSignals,\n",
    "    \"multimodal_divergence\": MultimodalSignals,\n",
    "}\n",
    "\n",
    "schema_cls = SCHEMA_MAP[PROMPT_NAME]\n",
    "modified_data = _extract_layer(client, modified_prompt, transcript_text)\n",
    "modified_result = schema_cls.model_validate(_coerce_to_schema(modified_data, schema_cls))\n",
    "print(f\"Modified extraction complete for layer: {PROMPT_NAME}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "original_prompt = load_prompt(PROMPT_NAME)\n",
    "original_data = _extract_layer(client, original_prompt, transcript_text)\n",
    "original_result = schema_cls.model_validate(_coerce_to_schema(original_data, schema_cls))\n",
    "print(f\"Original extraction complete for layer: {PROMPT_NAME}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(Markdown(\"## Side-by-Side Comparison\"))\n",
    "\n",
    "DISPLAY_FN = {\n",
    "    \"surface\": display_surface,\n",
    "    \"behavioral\": display_behavioral,\n",
    "    \"psychographic\": display_psychographic,\n",
    "    \"multimodal_divergence\": display_multimodal,\n",
    "}\n",
    "\n",
    "display(Markdown(\"### Original Prompt Result\"))\n",
    "DISPLAY_FN[PROMPT_NAME](original_result)\n",
    "\n",
    "display(Markdown(\"---\\n### Modified Prompt Result\"))\n",
    "DISPLAY_FN[PROMPT_NAME](modified_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(Markdown(\"### Diff Summary\"))\n",
    "\n",
    "if PROMPT_NAME == \"surface\":\n",
    "    orig_topics = {t.name.lower() for t in original_result.topics}\n",
    "    mod_topics = {t.name.lower() for t in modified_result.topics}\n",
    "    print(f\"Topics — Original: {len(orig_topics)}, Modified: {len(mod_topics)}\")\n",
    "    print(f\"  Added:   {mod_topics - orig_topics or '{none}'}\")\n",
    "    print(f\"  Removed: {orig_topics - mod_topics or '{none}'}\")\n",
    "    print(f\"  Shared:  {orig_topics & mod_topics}\")\n",
    "\n",
    "    orig_entities = {e.name.lower() for e in original_result.entities}\n",
    "    mod_entities = {e.name.lower() for e in modified_result.entities}\n",
    "    print(f\"\\nEntities — Original: {len(orig_entities)}, Modified: {len(mod_entities)}\")\n",
    "    print(f\"  Added:   {mod_entities - orig_entities or '{none}'}\")\n",
    "    print(f\"  Removed: {orig_entities - mod_entities or '{none}'}\")\n",
    "\n",
    "    print(f\"\\nAspects — Original: {len(original_result.aspects)}, Modified: {len(modified_result.aspects)}\")\n",
    "    print(f\"Key phrases — Original: {len(original_result.key_phrases)}, Modified: {len(modified_result.key_phrases)}\")\n",
    "\n",
    "elif PROMPT_NAME == \"behavioral\":\n",
    "    orig_types = {t.objection.type for t in original_result.objection_triples}\n",
    "    mod_types = {t.objection.type for t in modified_result.objection_triples}\n",
    "    print(f\"Objection types — Original: {orig_types}, Modified: {mod_types}\")\n",
    "    print(f\"  Added:   {mod_types - orig_types or '{none}'}\")\n",
    "    print(f\"  Removed: {orig_types - mod_types or '{none}'}\")\n",
    "    print(f\"\\nBuying intent markers — Original: {len(original_result.buying_intent_markers)}, Modified: {len(modified_result.buying_intent_markers)}\")\n",
    "    print(f\"Competitive mentions — Original: {len(original_result.competitive_mentions)}, Modified: {len(modified_result.competitive_mentions)}\")\n",
    "\n",
    "elif PROMPT_NAME == \"psychographic\":\n",
    "    print(f\"Mental model — Original: {original_result.mental_model.primary}, Modified: {modified_result.mental_model.primary}\")\n",
    "    orig_personas = {p.archetype for p in original_result.persona_indicators}\n",
    "    mod_personas = {p.archetype for p in modified_result.persona_indicators}\n",
    "    print(f\"Personas — Original: {orig_personas}, Modified: {mod_personas}\")\n",
    "\n",
    "else:\n",
    "    print(\"Use the side-by-side display above for detailed comparison.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Evaluation\n",
    "\n",
    "Compare extraction results against ground truth using the same metrics\n",
    "as `tests/test_extraction.py`.\n",
    "\n",
    "| Metric | Threshold | Description |\n",
    "|--------|-----------|-------------|\n",
    "| Topic recall | >= 50% | Overlap of extracted vs ground truth topics |\n",
    "| Entity recall | >= 50% | Overlap of extracted vs ground truth entities |\n",
    "| Objection type recall | >= 50% | Overlap of extracted vs ground truth objection types |\n",
    "| Buying intent presence | bool | If GT has markers, extraction should too |\n",
    "| Mental model accuracy | >= 50% | Primary mental model match across corpus |\n",
    "| Persona archetype overlap | bool | At least one archetype matches |\n",
    "| Multimodal divergence | bool | Divergences detected when GT has them |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_topic_recall(result: ExtractionResult, gt: ExtractionResult) -> float:\n",
    "    gt_topics = {t.name.lower() for t in gt.surface.topics}\n",
    "    extracted_topics = {t.name.lower() for t in result.surface.topics}\n",
    "    if not gt_topics:\n",
    "        return 1.0\n",
    "    return len(gt_topics & extracted_topics) / len(gt_topics)\n",
    "\n",
    "\n",
    "def compute_entity_recall(result: ExtractionResult, gt: ExtractionResult) -> float:\n",
    "    gt_entities = {e.name.lower() for e in gt.surface.entities}\n",
    "    extracted_entities = {e.name.lower() for e in result.surface.entities}\n",
    "    if not gt_entities:\n",
    "        return 1.0\n",
    "    return len(gt_entities & extracted_entities) / len(gt_entities)\n",
    "\n",
    "\n",
    "def compute_objection_type_recall(result: ExtractionResult, gt: ExtractionResult) -> float:\n",
    "    gt_types = {t.objection.type for t in gt.behavioral.objection_triples}\n",
    "    extracted_types = {t.objection.type for t in result.behavioral.objection_triples}\n",
    "    if not gt_types:\n",
    "        return 1.0\n",
    "    return len(gt_types & extracted_types) / len(gt_types)\n",
    "\n",
    "\n",
    "def check_buying_intent(result: ExtractionResult, gt: ExtractionResult) -> bool:\n",
    "    if not gt.behavioral.buying_intent_markers:\n",
    "        return True\n",
    "    return len(result.behavioral.buying_intent_markers) > 0\n",
    "\n",
    "\n",
    "def check_mental_model_match(result: ExtractionResult, gt: ExtractionResult) -> bool:\n",
    "    return result.psychographic.mental_model.primary == gt.psychographic.mental_model.primary\n",
    "\n",
    "\n",
    "def check_persona_overlap(result: ExtractionResult, gt: ExtractionResult) -> bool:\n",
    "    gt_archetypes = {p.archetype for p in gt.psychographic.persona_indicators}\n",
    "    extracted_archetypes = {p.archetype for p in result.psychographic.persona_indicators}\n",
    "    if not gt_archetypes:\n",
    "        return True\n",
    "    return bool(gt_archetypes & extracted_archetypes)\n",
    "\n",
    "\n",
    "def check_multimodal(result: ExtractionResult, gt: ExtractionResult) -> bool:\n",
    "    if gt.multimodal is None:\n",
    "        return result.multimodal is None\n",
    "    if gt.multimodal.divergences:\n",
    "        return result.multimodal is not None and len(result.multimodal.divergences) > 0\n",
    "    return True\n",
    "\n",
    "\n",
    "def evaluate_single(result: ExtractionResult, gt: ExtractionResult) -> dict:\n",
    "    \"\"\"Run all evaluation metrics on a single result/ground-truth pair.\"\"\"\n",
    "    return {\n",
    "        \"transcript_id\": result.transcript_id,\n",
    "        \"topic_recall\": compute_topic_recall(result, gt),\n",
    "        \"entity_recall\": compute_entity_recall(result, gt),\n",
    "        \"objection_type_recall\": compute_objection_type_recall(result, gt),\n",
    "        \"buying_intent_present\": check_buying_intent(result, gt),\n",
    "        \"mental_model_match\": check_mental_model_match(result, gt),\n",
    "        \"persona_overlap\": check_persona_overlap(result, gt),\n",
    "        \"multimodal_correct\": check_multimodal(result, gt),\n",
    "    }\n",
    "\n",
    "print(\"Evaluation functions loaded.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(Markdown(\"### Single Transcript Evaluation\"))\n",
    "\n",
    "if SELECTED_CALL_ID in ground_truths:\n",
    "    gt = ground_truths[SELECTED_CALL_ID]\n",
    "    metrics = evaluate_single(result, gt)\n",
    "\n",
    "    THRESHOLDS = {\n",
    "        \"topic_recall\": \">= 0.50\",\n",
    "        \"entity_recall\": \">= 0.50\",\n",
    "        \"objection_type_recall\": \">= 0.50\",\n",
    "        \"buying_intent_present\": \"True\",\n",
    "        \"mental_model_match\": \"True (>= 50% across corpus)\",\n",
    "        \"persona_overlap\": \"True\",\n",
    "        \"multimodal_correct\": \"True\",\n",
    "    }\n",
    "\n",
    "    rows = []\n",
    "    for key, value in metrics.items():\n",
    "        if key == \"transcript_id\":\n",
    "            continue\n",
    "        if isinstance(value, float):\n",
    "            display_val = f\"{value:.0%}\"\n",
    "            passed = value >= 0.5\n",
    "        else:\n",
    "            display_val = str(value)\n",
    "            passed = bool(value)\n",
    "        status = \"PASS\" if passed else \"FAIL\"\n",
    "        rows.append([key, display_val, THRESHOLDS.get(key, \"N/A\"), status])\n",
    "\n",
    "    display_table([\"Metric\", \"Value\", \"Threshold\", \"Status\"], rows)\n",
    "else:\n",
    "    print(f\"No ground truth available for {SELECTED_CALL_ID}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "display(Markdown(\"### Full Corpus Evaluation\"))\n",
    "print(\"This runs extraction on all transcripts with ground truth (~25 API calls).\\n\")\n",
    "\n",
    "corpus_results = []\n",
    "for call_id, t in transcripts.items():\n",
    "    if call_id not in ground_truths:\n",
    "        print(f\"Skipping {call_id} (no ground truth)\")\n",
    "        continue\n",
    "    print(f\"Extracting {call_id}...\", end=\" \", flush=True)\n",
    "    r = extract(t, client=client)\n",
    "    gt = ground_truths[call_id]\n",
    "    m = evaluate_single(r, gt)\n",
    "    corpus_results.append(m)\n",
    "    print(f\"done (topic_recall={m['topic_recall']:.0%}, entity_recall={m['entity_recall']:.0%})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(Markdown(\"### Per-Transcript Results\"))\n",
    "\n",
    "rows = []\n",
    "for m in corpus_results:\n",
    "    rows.append([\n",
    "        m[\"transcript_id\"],\n",
    "        f\"{m['topic_recall']:.0%}\",\n",
    "        f\"{m['entity_recall']:.0%}\",\n",
    "        f\"{m['objection_type_recall']:.0%}\",\n",
    "        str(m[\"buying_intent_present\"]),\n",
    "        str(m[\"mental_model_match\"]),\n",
    "        str(m[\"persona_overlap\"]),\n",
    "        str(m[\"multimodal_correct\"]),\n",
    "    ])\n",
    "display_table(\n",
    "    [\"Transcript\", \"Topic Recall\", \"Entity Recall\", \"Objection Recall\",\n",
    "     \"Buying Intent\", \"Mental Model\", \"Persona\", \"Multimodal\"],\n",
    "    rows,\n",
    ")\n",
    "\n",
    "# Aggregate metrics\n",
    "n = len(corpus_results)\n",
    "if n > 0:\n",
    "    avg_topic = sum(m[\"topic_recall\"] for m in corpus_results) / n\n",
    "    avg_entity = sum(m[\"entity_recall\"] for m in corpus_results) / n\n",
    "    avg_objection = sum(m[\"objection_type_recall\"] for m in corpus_results) / n\n",
    "    mental_model_accuracy = sum(1 for m in corpus_results if m[\"mental_model_match\"]) / n\n",
    "    all_persona = all(m[\"persona_overlap\"] for m in corpus_results)\n",
    "    all_intent = all(m[\"buying_intent_present\"] for m in corpus_results)\n",
    "    all_multimodal = all(m[\"multimodal_correct\"] for m in corpus_results)\n",
    "\n",
    "    display(Markdown(\"### Aggregate Metrics\"))\n",
    "    agg_rows = [\n",
    "        [\"Avg Topic Recall\", f\"{avg_topic:.0%}\", \">= 50%\", \"PASS\" if avg_topic >= 0.5 else \"FAIL\"],\n",
    "        [\"Avg Entity Recall\", f\"{avg_entity:.0%}\", \">= 50%\", \"PASS\" if avg_entity >= 0.5 else \"FAIL\"],\n",
    "        [\"Avg Objection Recall\", f\"{avg_objection:.0%}\", \">= 50%\", \"PASS\" if avg_objection >= 0.5 else \"FAIL\"],\n",
    "        [\"Mental Model Accuracy\", f\"{mental_model_accuracy:.0%}\", \">= 50%\", \"PASS\" if mental_model_accuracy >= 0.5 else \"FAIL\"],\n",
    "        [\"All Buying Intent\", str(all_intent), \"True\", \"PASS\" if all_intent else \"FAIL\"],\n",
    "        [\"All Persona Overlap\", str(all_persona), \"True\", \"PASS\" if all_persona else \"FAIL\"],\n",
    "        [\"All Multimodal Correct\", str(all_multimodal), \"True\", \"PASS\" if all_multimodal else \"FAIL\"],\n",
    "    ]\n",
    "    display_table([\"Metric\", \"Value\", \"Threshold\", \"Status\"], agg_rows)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Results Comparison\n",
    "\n",
    "Compare two extraction results against the same ground truth to measure\n",
    "the impact of prompt changes. Use this after running the Prompt Tuning\n",
    "Workbench to see how modifications affect evaluation metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_versions_vs_ground_truth(\n",
    "    result_a: ExtractionResult,\n",
    "    result_b: ExtractionResult,\n",
    "    gt: ExtractionResult,\n",
    "    label_a: str = \"Version A\",\n",
    "    label_b: str = \"Version B\",\n",
    "):\n",
    "    \"\"\"Compare two extraction results against the same ground truth.\"\"\"\n",
    "    metrics_a = evaluate_single(result_a, gt)\n",
    "    metrics_b = evaluate_single(result_b, gt)\n",
    "\n",
    "    rows = []\n",
    "    for key in metrics_a:\n",
    "        if key == \"transcript_id\":\n",
    "            continue\n",
    "        val_a = metrics_a[key]\n",
    "        val_b = metrics_b[key]\n",
    "        if isinstance(val_a, float):\n",
    "            disp_a = f\"{val_a:.0%}\"\n",
    "            disp_b = f\"{val_b:.0%}\"\n",
    "            delta = val_b - val_a\n",
    "            delta_str = f\"{delta:+.0%}\"\n",
    "        else:\n",
    "            disp_a = str(val_a)\n",
    "            disp_b = str(val_b)\n",
    "            delta_str = \"same\" if val_a == val_b else \"changed\"\n",
    "        rows.append([key, disp_a, disp_b, delta_str])\n",
    "\n",
    "    display(Markdown(f\"### {label_a} vs {label_b}\"))\n",
    "    display_table([\"Metric\", label_a, label_b, \"Delta\"], rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: compare two full extraction results against ground truth.\n",
    "# Replace result_a and result_b with your own ExtractionResult objects.\n",
    "#\n",
    "# To compare prompt tuning results, run a full extraction with each prompt:\n",
    "#   result_original = extract(transcript, client=client)\n",
    "#   # ... modify the prompt file on disk, or rebuild the extractor ...\n",
    "#   result_modified = extract(transcript, client=client)\n",
    "#\n",
    "#   compare_versions_vs_ground_truth(\n",
    "#       result_original, result_modified,\n",
    "#       ground_truths[SELECTED_CALL_ID],\n",
    "#       \"Original\", \"Modified\"\n",
    "#   )\n",
    "\n",
    "print(\"Use compare_versions_vs_ground_truth(result_a, result_b, gt) to compare two results.\")\n",
    "print(f\"Ground truth available for: {list(ground_truths.keys())}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "source": "---\n## Evaluation Module — Precision, Recall & F1\n\nThe evaluation module (`customer_intelligence.evaluation`) provides a comprehensive\nevaluation pipeline with three methods:\n\n1. **Programmatic metrics** — Precision, recall, F1 via fuzzy matching for all signal types\n2. **LLM-as-judge** — Rubric-based quality scoring for subjective signals (optional)\n3. **NLP baselines** — Cross-validation with spaCy NER, YAKE keyphrases, TextBlob sentiment (optional)\n\nThis replaces the simpler recall-only evaluation above with per-signal-type precision AND recall,\nfuzzy matching (so \"ROI justification\" matches \"ROI analysis\"), and structural checks.\n\nSee `docs/evaluation.md` for full details on the methodology.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "from customer_intelligence.evaluation import evaluate, evaluate_corpus\nfrom customer_intelligence.evaluation.report import EvaluationReport, CorpusReport\n\nprint(\"Evaluation module loaded.\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# Single transcript evaluation with the evaluation module\n# Uses 'result' from the extraction above and ground truth\nif SELECTED_CALL_ID in ground_truths:\n    gt = ground_truths[SELECTED_CALL_ID]\n    report = evaluate(result, gt, transcript, skip_llm_judge=True)\n    print(report.summary())\nelse:\n    print(f\"No ground truth for {SELECTED_CALL_ID} — skipping evaluation\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# Corpus-level evaluation with per-signal-type breakdown\ncases = []\nfor call_id, t in transcripts.items():\n    if call_id not in ground_truths:\n        continue\n    gt = ground_truths[call_id]\n    # Re-use existing extraction if available, otherwise extract\n    if call_id in extractions:\n        ex = extractions[call_id]\n    elif call_id == SELECTED_CALL_ID:\n        ex = result\n    else:\n        print(f\"Extracting {call_id}...\", end=\" \", flush=True)\n        ex = extract(t, client=client)\n        print(\"done\")\n    cases.append((ex, gt, t))\n\nprint(f\"\\nEvaluating {len(cases)} transcripts...\")\ncorpus = evaluate_corpus(cases, skip_llm_judge=True)\n\n# Per-signal-type aggregated metrics\ndisplay(Markdown(\"### Corpus Evaluation — Per-Signal Metrics\"))\nagg = corpus.mean_metrics_by_signal()\nrows = []\nfor signal, stats in sorted(agg.items()):\n    p = stats.get(\"precision\")\n    r = stats.get(\"recall\")\n    f = stats.get(\"f1\")\n    rows.append([\n        signal,\n        f\"{p:.0%}\" if p is not None else \"—\",\n        f\"{r:.0%}\" if r is not None else \"—\",\n        f\"{f:.0%}\" if f is not None else \"—\",\n    ])\ndisplay_table([\"Signal\", \"Precision\", \"Recall\", \"F1\"], rows)\n\n# Per-transcript summary\ndisplay(Markdown(\"### Per-Transcript Overall F1\"))\nfor report in corpus.reports:\n    print(f\"  {report.transcript_id}: F1 = {report.overall_f1:.2%}\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}